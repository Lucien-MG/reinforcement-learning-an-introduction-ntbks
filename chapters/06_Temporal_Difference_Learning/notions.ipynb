{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Difference Learning\n",
    "\n",
    "Temporal-difference (TD) learning is a combination of Monte-carlo ideas and dynamic programming (DP) ideas.\n",
    "\n",
    "TD is:\n",
    "- Like Monte Carlo, TD can learn directly from raw experience without a model of the environment's dynamics\n",
    "- Like DP, TD methods update estimates based in part on other learned estimates, without waiting for final outcomes (they bootstrap)\n",
    "\n",
    "## Temporal-Learning Prediction\n",
    "\n",
    "Both TD and MC methods use experience to solve prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class TemporalLearningZeroPrediction():\n",
    "\n",
    "    def __init__(self, gamma, alpha, policy):\n",
    "        self.gamma  = gamma\n",
    "        self.alpha  = alpha\n",
    "        self.policy = policy\n",
    "\n",
    "        self.state_value = collections.defaultdict(lambda: 0)\n",
    "        self.returns = collections.defaultdict(lambda: 0)\n",
    "\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def action(self, state):\n",
    "        return self.policy(state)\n",
    "    \n",
    "    def observe(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def optimize(self):\n",
    "        g = 0\n",
    "\n",
    "        for t in reversed(range(len(self.states))):\n",
    "            g = self.gamma * g + self.rewards[t]\n",
    "\n",
    "            if not self.states[t] in self.states[0:t]:\n",
    "                self.returns[self.states[t]] += 1\n",
    "                self.state_value[self.states[t]] += (1 / self.returns[self.states[t]]) * (g - self.state_value[self.states[t]])\n",
    "        \n",
    "        self.states = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa: On-policy TD Control\n",
    "\n",
    "Following the pattern of generalized policy iteration (GPI), using TD methods for the evaluation or prediction part.  \n",
    "Again, we need to trade off exploration and exploitation, with two approaches: on-policy and off-policy.\n",
    "\n",
    "The theorems assuring the convergence of state values under TD(0) also apply to the corresponding algorithm for action values:\n",
    "\n",
    "$ Q(S_t, A_t) = Q(S_t, A_t) + \\alpha * [R_{t+1} + \\gamma * Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Sarsa():\n",
    "\n",
    "    def __init__(self, action_space, gamma, alpha, policy):\n",
    "        self.gamma  = gamma\n",
    "        self.alpha  = alpha\n",
    "        self.policy = policy\n",
    "\n",
    "        self.state_action_value = collections.defaultdict(lambda: np.zeros((action_space.n)))\n",
    "        self.selected_action = None\n",
    "\n",
    "    def action(self, state):\n",
    "        if self.selected_action == None:\n",
    "            return self.policy(self.state_action_value, state)\n",
    "        return self.selected_action\n",
    "    \n",
    "    def observe(self, state, action, reward, next_state):\n",
    "        next_action = self.policy(self.state_action_value, next_state)\n",
    "        target = reward + self.gamma * self.state_action_value[next_state][next_action]\n",
    "\n",
    "        self.state_action_value[state][action] += self.alpha * (target - self.state_action_value[state][action])\n",
    "\n",
    "        self.selected_action = next_action\n",
    "    \n",
    "    def optimize(self):\n",
    "        self.selected_action = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Human():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def action(self, state):\n",
    "        return int(input())\n",
    "    \n",
    "    def observe(self, state, action, reward, next_state):\n",
    "        pass\n",
    "    \n",
    "    def optimize(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windy Gridworld Env\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class Actions(Enum):\n",
    "    RIGHT = 0\n",
    "    UP = 1\n",
    "    LEFT = 2\n",
    "    DOWN = 3\n",
    "\n",
    "class WindyGridworld(gym.Env):\n",
    "    metadata = { \"render_modes\": [\"ascii\"] }\n",
    "\n",
    "    def __init__(self, render_mode=None, grid_shape=(7, 10)):\n",
    "        self._grid_shape = grid_shape\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, self._grid_shape[0] - 1, shape=(2,), dtype=int),\n",
    "                \"target\": spaces.Box(0, self._grid_shape[0] - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self._agent_location = np.array([3, 0], dtype=int)\n",
    "        self._target_location = np.array([3, 7], dtype=int)\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        i.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            Actions.DOWN.value: np.array([1, 0]),\n",
    "            Actions.RIGHT.value: np.array([0, 1]),\n",
    "            Actions.UP.value: np.array([-1, 0]),\n",
    "            Actions.LEFT.value: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return str(self._agent_location)\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "            self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def _render_frame(self):\n",
    "        if self.render_mode == \"ascii\":\n",
    "            grid = np.zeros((7, 10))\n",
    "            grid[self._agent_location[0], self._agent_location[1]] = 1\n",
    "            grid[self._target_location[0], self._target_location[1]] = 6\n",
    "            print(grid, flush=True)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        wind_force = 0\n",
    "\n",
    "        if self._agent_location[1] in [3, 4, 5, 8]:\n",
    "            wind_force = 1 # apply wind with force of 1\n",
    "\n",
    "        if self._agent_location[1] in [6, 7]:\n",
    "            wind_force = 2 # apply wind with force of 2\n",
    "\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location[0] = np.clip(\n",
    "            self._agent_location[0] + direction[0] - wind_force, 0, self._grid_shape[0] - 1\n",
    "        )\n",
    "\n",
    "        self._agent_location[1] = np.clip(\n",
    "            self._agent_location[1] + direction[1], 0, self._grid_shape[1] - 1\n",
    "        )\n",
    "\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        terminated = np.all(self._agent_location == self._target_location)\n",
    "        reward = 0 if terminated else -1\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # reset agent's position\n",
    "        self._agent_location = np.array([3, 0], dtype=int)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        self._render_frame()\n",
    "\n",
    "        return observation, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessaries libraries\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WindyGridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_env(env, agent):\n",
    "    reward_sum = 0\n",
    "    nb_steps = 0\n",
    "\n",
    "    terminated = False\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    while not terminated:\n",
    "        action = agent.action(observation)\n",
    "\n",
    "        new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        agent.observe(observation, action, reward, new_observation)\n",
    "\n",
    "        observation = new_observation\n",
    "\n",
    "        reward_sum += reward\n",
    "        nb_steps += 1\n",
    "    \n",
    "    agent.optimize()\n",
    "\n",
    "    return reward_sum, nb_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(array):\n",
    "    return np.random.choice(np.where(array == np.max(array))[0])\n",
    "\n",
    "def get_epsilon_greedy_policy(epsilon=0.1):\n",
    "    def epsilon_greedy_policy(state_action_value, state):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < epsilon:\n",
    "            random_action = np.random.randint(0, len(state_action_value[state]))\n",
    "            return random_action\n",
    "        else:\n",
    "            greedy_action = argmax(state_action_value[state])\n",
    "            return greedy_action\n",
    "    \n",
    "    return epsilon_greedy_policy\n",
    "\n",
    "agent = Sarsa(env.action_space, alpha=0.5, gamma=1, policy=get_epsilon_greedy_policy(epsilon=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "time_steps = [0]\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "    reward, steps = play_env(env, agent)\n",
    "\n",
    "    rewards.append(reward)\n",
    "    time_steps.append(time_steps[-1] + steps)\n",
    "\n",
    "print(agent.state_action_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=1, cols=1, subplot_titles=\"Windy Gridworld\")\n",
    "\n",
    "x = np.array(time_steps)\n",
    "y = np.arange(len(time_steps))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        line_color=\"red\",\n",
    "        name=\"test\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"test\",\n",
    "    legend_title=\"Parameters\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
