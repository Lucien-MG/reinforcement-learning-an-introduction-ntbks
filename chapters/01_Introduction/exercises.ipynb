{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Exercises\n",
    "\n",
    "### Exercise 1.1: **Self-Play**\n",
    "\n",
    "**Question: Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides    \n",
    "learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?**\n",
    "\n",
    "> If the reinforcement learning algorithm played against itself (self-play), it would likely learn a different policy for selecting moves.  \n",
    "We can anwser that because self-play introduces a dynamic and adaptive opponent.  \n",
    "The agent must constantly adapt to the evolving strategies of its own \"other half.\"  \n",
    "This might lead to infinite policy changes that never converge or sub-optimal policy.  \n",
    "\n",
    "\n",
    "### Exercise 1.2: **Symmetries**\n",
    "\n",
    "**Question: Many tic-tac-toe positions appear different but are really the same because of symmetries.  \n",
    "How might we amend the learning process described above to take advantage of this?**\n",
    "\n",
    "> We could use symetric approach using a dictionnary that store canonicalize states,  \n",
    "this could involve rotating or flipping the board to a standard orientation.  \n",
    "Before updating the value function or policy, transform each board position into a \"canonical\" representation.  \n",
    "Or we could use appromximation method (neural network) to get some generalisation.  \n",
    "\n",
    "**Question: In what ways would this change improve the learning\n",
    "process?**\n",
    "\n",
    "> This could dramaticaly improve learning efficiency since learning one situation\n",
    "will impact many others.\n",
    "\n",
    "**Question: Now think again. Suppose the opponent did not take advantage of symmetries.\n",
    "In that case, should we ?**\n",
    "\n",
    "> Probably yes, since we could take advantage of previously explored sitatuation\n",
    "\n",
    "**Question: Is it true, then, that symmetrically equivalent positions should\n",
    "necessarily have the same value?**\n",
    "\n",
    "> In an optimal policy yes. In the case where the agent doesn't learn in a symmetric way and  \n",
    "is training, the policy could have different values for equivalent positions.\n",
    "\n",
    "\n",
    "### Exercise 1.3: **Greedy Play**\n",
    "\n",
    "**Question: Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best.  \n",
    "Might it learn to play better, or worse, than a nongreedy player ?**\n",
    "\n",
    "> It might learn to avoid bad move but never find the best ones. The greedy agent will be stuck in a sub optimal policy.  \n",
    "Without occasional exploration, the player might never discover better moves or improve its understanding of the game's dynamics.  \n",
    "In the long run, a purely greedy player in Tic-Tac-Toe will likely not perform better than a non-greedy player.  \n",
    "\n",
    "**Question: What problems might occur?**\n",
    "\n",
    "> A greedy player always chooses the move it currently believes to be the best.  \n",
    "This prevents it from exploring alternative moves, even if those moves might lead to better long-term outcomes.  \n",
    "Then the greedy approach can easily get stuck in local optima.  \n",
    "See trade off between exploration vs exploitation.  \n",
    "\n",
    "### Exercise 1.4: **Learning from Exploration**\n",
    "\n",
    "**Question: Suppose learning updates occurred after all moves, including exploratory moves.  \n",
    "If the step-size parameter is appropriately reduced over time (but not the tendency to explore),  \n",
    "then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities  \n",
    "computed when we do, and when we do not, learn from exploratory moves ?**\n",
    "\n",
    "> TODO\n",
    "\n",
    "**Question: Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn ?**\n",
    "\n",
    "> TODO\n",
    "\n",
    "**Question: Which would result in more wins?**\n",
    "\n",
    "> TODO\n",
    "\n",
    "### Exercise 1.5: **Other Improvements**\n",
    "\n",
    "**Question: Can you think of other ways to improve the reinforcement learning player ?**\n",
    "\n",
    "> TODO\n",
    "\n",
    "**Question: Can you think of any better way to solve the tic-tac-toe problem as posed?**\n",
    "\n",
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
