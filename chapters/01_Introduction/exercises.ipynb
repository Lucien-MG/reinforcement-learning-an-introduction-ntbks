{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Exercises\n",
    "\n",
    "### Exercise 1.1: **Self-Play**\n",
    "\n",
    "**Question: Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides    \n",
    "learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?**\n",
    "\n",
    "> When a reinforcement learning agent engages in self-play, the resulting policy is likely to diverge from one learned against a static opponent.  \n",
    "This is due to the inherent dynamism of self-play, where the agent's 'opponent' is itself, evolving its strategy with each iteration.  \n",
    "Consequently, the agent must continuously adapt, potentially leading to non-convergence or suboptimal policies.\n",
    "\n",
    "\n",
    "### Exercise 1.2: **Symmetries**\n",
    "\n",
    "**Question: Many tic-tac-toe positions appear different but are really the same because of symmetries.  \n",
    "How might we amend the learning process described above to take advantage of this?**\n",
    "\n",
    "> We could use symetric approach using a dictionnary that store canonicalize states,  \n",
    "this could involve rotating or flipping the board to a standard orientation.  \n",
    "Before updating the value function or policy, transform each board position into a \"canonical\" representation.  \n",
    "Or we could use appromximation method (neural network) to get some generalisation.  \n",
    "\n",
    "**Question: In what ways would this change improve the learning\n",
    "process?**\n",
    "\n",
    "> This could dramaticaly improve learning efficiency since learning one situation\n",
    "will impact many others.\n",
    "\n",
    "**Question: Now think again. Suppose the opponent did not take advantage of symmetries.\n",
    "In that case, should we ?**\n",
    "\n",
    "> Probably yes, since we could take advantage of previously explored sitatuation\n",
    "\n",
    "**Question: Is it true, then, that symmetrically equivalent positions should\n",
    "necessarily have the same value?**\n",
    "\n",
    "> In an optimal policy yes. In the case where the agent doesn't learn in a symmetric way and  \n",
    "is training, the policy could have different values for equivalent positions.\n",
    "\n",
    "\n",
    "### Exercise 1.3: **Greedy Play**\n",
    "\n",
    "**Question: Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best.  \n",
    "Might it learn to play better, or worse, than a nongreedy player ?**\n",
    "\n",
    "> It might learn to avoid bad move but never find the best ones. The greedy agent will be stuck in a sub optimal policy.  \n",
    "Without occasional exploration, the player might never discover better moves or improve its understanding of the game's dynamics.  \n",
    "In the long run, a purely greedy player in Tic-Tac-Toe will likely not perform better than a non-greedy player.  \n",
    "\n",
    "**Question: What problems might occur?**\n",
    "\n",
    "> A greedy player always chooses the move it currently believes to be the best.  \n",
    "This prevents it from exploring alternative moves, even if those moves might lead to better long-term outcomes.  \n",
    "Then the greedy approach can easily get stuck in local optima.  \n",
    "See trade off between exploration vs exploitation.  \n",
    "\n",
    "### Exercise 1.4: **Learning from Exploration**\n",
    "\n",
    "**Question: Suppose learning updates occurred after all moves, including exploratory moves.  \n",
    "If the step-size parameter is appropriately reduced over time (but not the tendency to explore),  \n",
    "then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities  \n",
    "computed when we do, and when we do not, learn from exploratory moves ?**\n",
    "\n",
    "> The two sets of probabilites are:\n",
    "Learning only from optimal actions which provides a more accurate representation of the true potential of each state.  \n",
    "Learning from all actions which reflects the agent's actual experience, which may include suboptimal choices and exploration.\n",
    "\n",
    "**Question: Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn ?**\n",
    "\n",
    "> Learning from all actions (including exploratory moves) might be more beneficial,  \n",
    "since good move and new strategies can be discovered. Otherwise, the agent would only learn  \n",
    "what is the best action from a small moves part.\n",
    "\n",
    "**Question: Which would result in more wins?**\n",
    "\n",
    "> In the end, probably the agent exploring will probably win more since best moves can be discovered.\n",
    "\n",
    "### Exercise 1.5: **Other Improvements**\n",
    "\n",
    "**Question: Can you think of other ways to improve the reinforcement learning player ?**\n",
    "\n",
    "> Reinforcement learning agents can improve by playing against a large pool of human players online. Alternatively, they can learn through self-play,  \n",
    "where they compete against a static version of themselves. Once the agent consistently surpasses this static version,  \n",
    "its current state can be 'frozen' and used as a new baseline for further self-play.\n",
    "\n",
    "**Question: Can you think of any better way to solve the tic-tac-toe problem as posed?**\n",
    "\n",
    "> Given Tic-Tac-Toe's simplicity, more efficient solutions exist.  \n",
    "Minimax, for instance, effectively determines winning or stalemate moves by exploring the limited game states."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
