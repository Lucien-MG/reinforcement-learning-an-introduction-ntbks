{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Markov Decision Processes\n",
    "\n",
    "Markov Decision Processes (MDPs) are a classical formalization of sequential decision making,  \n",
    "where actions influence not just immediate rewards, but also subsequent situations,  \n",
    "or states, and through those future rewards.  \n",
    "\n",
    "Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward.  \n",
    "Whereas in bandit problems we estimated the value $q_*(a)$ of each action $a$,  \n",
    "in MDPs we estimate the value $q_*(s, a)$ of each action a in each state $s$,  \n",
    "or we estimate the value $v_*(s)$ of each state given optimal action selections.\n",
    "\n",
    "\n",
    "These state-dependent quantities are essential to accurately assigning credit for long-term\n",
    "consequences to individual action selections.\n",
    "\n",
    "MDPs are a mathematically idealized form of the reinforcement learning problem  \n",
    "for which precise theoretical statements can be made. We introduce key elements of  \n",
    "the problemâ€™s mathematical structure, such as returns, value functions, and Bellman  \n",
    "equations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
