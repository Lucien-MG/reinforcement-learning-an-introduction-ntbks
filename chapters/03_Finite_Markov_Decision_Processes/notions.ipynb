{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Markov Decision Processes\n",
    "\n",
    "**Markov Decision Processes** (MDPs) are a classical formalization of sequential decision making,  \n",
    "where actions influence not just immediate rewards, but also subsequent situations,  \n",
    "or states, and through those future rewards.  \n",
    "\n",
    "Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward.  \n",
    "Whereas in bandit problems we estimated the value $q_*(a)$ of each action $a$,  \n",
    "in MDPs we estimate the value $q_*(s, a)$ of each action a in each state $s$,  \n",
    "or we estimate the value $v_*(s)$ of each state given optimal action selections.\n",
    "\n",
    "These state-dependent quantities are essential to accurately assigning credit for long-term  \n",
    "consequences to individual action selections.\n",
    "\n",
    "MDPs are a mathematically idealized form of the reinforcement learning problem  \n",
    "for which precise theoretical statements can be made. We introduce key elements of  \n",
    "the problem’s mathematical structure, such as returns, value functions, and Bellman  \n",
    "equations.\n",
    "\n",
    "## The Agent–Environment Interface\n",
    "\n",
    "MDPs are meant to be a straightforward framing of the problem of learning from  \n",
    "interaction to achieve a goal. The learner and decision maker is called the **agent**.  \n",
    "The thing it interacts with, comprising everything outside the agent, is called the **environment**.  \n",
    "\n",
    "These interact continually, the agent selecting actions and the environment responding to  \n",
    "these actions and presenting new situations to the agent. The environment also gives  \n",
    "rise to rewards, special numerical values that the agent seeks to maximize over time  \n",
    "through its choice of actions.\n",
    "\n",
    "At each time step $t = 0, 1, 2, 3...$ there is a sequence (or a **trajectory**):\n",
    "\n",
    "$S_0$ -> agent -> $A_0$ -> env -> $R_1$, $S_1$ -> agent -> $A_1$ -> env -> ...\n",
    "\n",
    "In a finite MDP, the sets of states, actions, and rewards ($S$, $A$, and $R$) all have a finite  \n",
    "number of elements. In this case, the random variables R_t and S_t have well defined  \n",
    "discrete probability distributions dependent only on the preceding state and action.  \n",
    "That is, for particular values of these random variables, $s' \\ S$ and r 2 R, there is a probability  \n",
    "of those values occurring at time t, given particular values of the preceding state and\n",
    "action:\n",
    "\n",
    "$p(s', r | s, a) = Pr\\{S_t=s', R_t = r | S_{t-1} = s, A_{t-1} = a\\}$  \n",
    "$\\forall s', s \\in S, \\forall r \\in R, \\forall a \\in A(s)$\n",
    "\n",
    "The function p defines the **dynamics** of the MDP.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
