{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Markov Decision Processes\n",
    "\n",
    "**Markov Decision Processes** (MDPs) are a classical formalization of sequential decision making,  \n",
    "where actions influence not just immediate rewards, but also subsequent situations,  \n",
    "or states, and through those future rewards.  \n",
    "\n",
    "Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward.  \n",
    "Whereas in bandit problems we estimated the value $q_*(a)$ of each action $a$,  \n",
    "in MDPs we estimate the value $q_*(s, a)$ of each action a in each state $s$,  \n",
    "or we estimate the value $v_*(s)$ of each state given optimal action selections.\n",
    "\n",
    "These state-dependent quantities are essential to accurately assigning credit for long-term  \n",
    "consequences to individual action selections.\n",
    "\n",
    "MDPs are a mathematically idealized form of the reinforcement learning problem  \n",
    "for which precise theoretical statements can be made. We introduce key elements of  \n",
    "the problem’s mathematical structure, such as returns, value functions, and Bellman  \n",
    "equations.\n",
    "\n",
    "## The Agent–Environment Interface\n",
    "\n",
    "MDPs are meant to be a straightforward framing of the problem of learning from  \n",
    "interaction to achieve a goal. The learner and decision maker is called the **agent**.  \n",
    "The thing it interacts with, comprising everything outside the agent, is called the **environment**.  \n",
    "\n",
    "These interact continually, the agent selecting actions and the environment responding to  \n",
    "these actions and presenting new situations to the agent. The environment also gives  \n",
    "rise to rewards, special numerical values that the agent seeks to maximize over time  \n",
    "through its choice of actions.\n",
    "\n",
    "At each time step $t = 0, 1, 2, 3...$ there is a sequence (or a **trajectory**):\n",
    "\n",
    "$S_0$ -> agent -> $A_0$ -> env -> $R_1$, $S_1$ -> agent -> $A_1$ -> env -> ...\n",
    "\n",
    "In a finite MDP, the sets of states, actions, and rewards ($S$, $A$, and $R$) all have a finite  \n",
    "number of elements. In this case, the random variables R_t and S_t have well defined  \n",
    "discrete probability distributions dependent only on the preceding state and action.  \n",
    "That is, for particular values of these random variables, $s' \\ S$ and r 2 R, there is a probability  \n",
    "of those values occurring at time t, given particular values of the preceding state and\n",
    "action:\n",
    "\n",
    "$p(s', r | s, a) = Pr\\{S_t=s', R_t = r | S_{t-1} = s, A_{t-1} = a\\}$  \n",
    "$\\forall s', s \\in S, \\forall r \\in R, \\forall a \\in A(s)$\n",
    "\n",
    "The function p defines the **dynamics** of the MDP.\n",
    "\n",
    "The dynamics function p : S x R x S x A ⇥ [0, 1] is an ordinary deterministic function of four arguments.  \n",
    "The ‘|’ in the middle of it comes from the notation for conditional probability,  \n",
    "but here it just reminds us that p specifies a probability distribution for each choice of s and a, that is, that  \n",
    "\n",
    "$\\sum_{s'=S} \\sum_{r=R} p(s', r | s, a) = 1, \\forall s \\in S, \\forall a \\in A(s)$\n",
    "\n",
    "In a Markov decision process, the probabilities given by p completely characterize the\n",
    "environment’s dynamics.\n",
    "\n",
    "The state must include information about all aspects of the past agent–environment interaction that make a difference for the future.  \n",
    "If it does, then the state is said to have the **Markov property**.\n",
    "\n",
    "From the four-argument dynamics function, p, one can compute anything else one might\n",
    "want to know about the environment, such as the state-transition probabilities (which we\n",
    "denote, with a slight abuse of notation, as a three-argument function p : S x S x A ⇥ [0, 1]):\n",
    "\n",
    "State-Transition probabilities:\n",
    "\n",
    "$p(s' | s, a) = Pr\\{S_t = s' | S_{t-1} = s, A_{t-1} = a\\} = \\sum_{r \\in R} p(s', r | s, a), \\forall s \\in S, \\forall a \\in A(s)$\n",
    "\n",
    "Expected Rewards for state-action:\n",
    "\n",
    "$r(s, a) = E\\{R_t | S_{t-1} = s, A_{t-1} = a\\} = \\sum_{r \\in R} r \\sum_{s' \\in S} p(s', r | s, a), \\forall s \\in S, \\forall a \\in A(s)$\n",
    "\n",
    "State action next state:\n",
    "\n",
    "$r(s, a, s') = E\\{R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'\\} = \\sum_{r \\in R} r * { p(s', r | s, a) \\over p(s' | s, a)}, \\forall s \\in S, \\forall a \\in A(s)$\n",
    "\n",
    "The MDP framework is abstract and flexible and can be applied to many di↵erent\n",
    "problems in many different ways."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
