{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Markov Decision Processes\n",
    "\n",
    "**Markov Decision Processes** (MDPs) (also called **stochastic dynamic program** or **stochastic control problem**)  \n",
    "are a classical formalization of sequential decision making when outcomes are uncertain,  \n",
    "where actions influence not just immediate rewards, but also subsequent situations,  \n",
    "or states, and through those future rewards. The name come from it's connection to **Markov chains (MCs) by Andrey Markov**.\n",
    "\n",
    "Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward.  \n",
    "Whereas in bandit problems we estimated the value $q_*(a)$ of each action $a$,  \n",
    "in MDPs we estimate the value $q_*(s, a)$ of each action a in each state $s$,  \n",
    "or we estimate the value $v_*(s)$ of each state given optimal action selections.\n",
    "\n",
    "These state-dependent quantities are essential to accurately assigning credit for long-term  \n",
    "consequences to individual action selections.\n",
    "\n",
    "MDPs are a mathematically idealized form of the reinforcement learning problem  \n",
    "for which precise theoretical statements can be made. We introduce key elements of  \n",
    "the problem’s mathematical structure, such as returns, value functions, and Bellman  \n",
    "equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent–Environment Interface\n",
    "\n",
    "MDPs are meant to be a straightforward framing of the problem of learning from  \n",
    "interaction to achieve a goal. The learner and decision maker is called the **agent**.  \n",
    "The thing it interacts with, comprising everything outside the agent, is called the **environment**.  \n",
    "\n",
    "These interact continually, the agent selecting actions and the environment responding to  \n",
    "these actions and presenting new situations to the agent. The environment also gives  \n",
    "rise to rewards, special numerical values that the agent seeks to maximize over time  \n",
    "through its choice of actions.\n",
    "\n",
    "At each time step $t = 0, 1, 2, 3...$ there is a **sequence** (or a **trajectory**):\n",
    "\n",
    "$S_0$ -> agent -> $A_0$ -> env -> $R_1$, $S_1$ -> agent -> $A_1$ -> env -> ...\n",
    "\n",
    "In a finite MDP, the sets of states, actions, and rewards ($S$, $A$, and $R$) all have a finite  \n",
    "number of elements. In this case, the random variables R_t and S_t have well defined  \n",
    "discrete probability distributions dependent only on the preceding state and action.  \n",
    "That is, for particular values of these random variables, $s' \\ S$ and r 2 R, there is a probability  \n",
    "of those values occurring at time t, given particular values of the preceding state and\n",
    "action:\n",
    "\n",
    "$p(s', r | s, a) = Pr\\{S_t=s', R_t = r | S_{t-1} = s, A_{t-1} = a\\}$  \n",
    "$\\forall s', s \\in S, \\forall r \\in R, \\forall a \\in A(s)$\n",
    "\n",
    "The function p defines the **dynamics** of the MDP.\n",
    "\n",
    "The dynamics function p : S x R x S x A ⇥ [0, 1] is an ordinary deterministic function of four arguments.  \n",
    "The ‘|’ in the middle of it comes from the notation for conditional probability,  \n",
    "but here it just reminds us that p specifies a probability distribution for each choice of s and a, that is, that  \n",
    "\n",
    "$\\sum_{s'=S} \\sum_{r=R} p(s', r | s, a) = 1, \\forall s \\in S, \\forall a \\in A(s)$\n",
    "\n",
    "In a Markov decision process, the probabilities given by p completely characterize the\n",
    "environment’s dynamics.\n",
    "\n",
    "The state must include information about all aspects of the past agent–environment interaction that make a difference for the future.  \n",
    "If it does, then the state is said to have the **Markov property**.\n",
    "\n",
    "From the four-argument dynamics function, p, one can compute anything else one might\n",
    "want to know about the environment, such as the state-transition probabilities (which we\n",
    "denote, with a slight abuse of notation, as a three-argument function p : S x S x A ⇥ [0, 1]):\n",
    "\n",
    "State-Transition probabilities:\n",
    "\n",
    "$p(s' | s, a) = Pr\\{S_t = s' | S_{t-1} = s, A_{t-1} = a\\} = \\sum_{r \\in R} p(s', r | s, a), \\forall s \\in S, \\forall a \\in A(s)$\n",
    "\n",
    "Expected Rewards for state-action:\n",
    "\n",
    "$r(s, a) = E\\{R_t | S_{t-1} = s, A_{t-1} = a\\} = \\sum_{r \\in R} r \\sum_{s' \\in S} p(s', r | s, a), \\forall s \\in S, \\forall a \\in A(s)$\n",
    "\n",
    "State action next state:\n",
    "\n",
    "$r(s, a, s') = E\\{R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'\\} = \\sum_{r \\in R} r * { p(s', r | s, a) \\over p(s' | s, a)}, \\forall s \\in S, \\forall a \\in A(s)$\n",
    "\n",
    "The MDP framework is abstract and flexible and can be applied to many di↵erent\n",
    "problems in many different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals and Rewards\n",
    "\n",
    "In reinforcement learning, the purpose or goal of the agent is formalized in terms of a\n",
    "special signal, called the reward, passing from the environment to the agent.\n",
    "At each time\n",
    "step, the reward is a simple number, Rt 2 R. Informally, the agent’s goal is to maximize\n",
    "the total amount of reward it receives. This means maximizing not immediate reward,\n",
    "but cumulative reward in the long run. We can clearly state this informal idea as the\n",
    "reward hypothesis:\n",
    "\n",
    "\"\"\"\n",
    "That all of what we mean by goals and purposes can be well thought of as\n",
    "the maximization of the expected value of the cumulative sum of a received\n",
    "scalar signal (called reward).\n",
    "\"\"\"\n",
    "\n",
    "The use of a reward signal to formalize the idea of a goal is one of the most distinctive\n",
    "features of reinforcement learning.\n",
    "\n",
    "The reward signal is your way of communicating to\n",
    "the agent what you want it to achieve, not how you want it achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns and episodes\n",
    "\n",
    "The goal of the agent is to maximize the cumulative reward.\n",
    "The cumulative reward is defined as:\n",
    "\n",
    "$G_t = R_{t+1} + R_{t+2} + R_{t+3} + R_{t+4} + ... + R_{T}$\n",
    "\n",
    "where T is a final time step.\n",
    "\n",
    "this works well for **episodic tasks** with a **terminal state**\n",
    "\n",
    "all non terminal state are denoted S and all state + terminal state are denoted S+.\n",
    "\n",
    "When the agent environment interaction does not break into identifiable episodes,\n",
    "it is a continuing task. with $T = \\infty$\n",
    "\n",
    "The additional concept that we need is that of **discounting**. \n",
    "\n",
    "According to this approach,\n",
    "the agent tries to select actions so that the sum of the discounted rewards it receives over\n",
    "the future is maximized. In particular, it chooses At to maximize the expected **discounted\n",
    "return**:\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma * R_{t+2} + + \\gamma ² * R_{t+2} + ... = \\sum_{k=0}^{\\infty} \\gamma ^k R_{t+k+1}$\n",
    "\n",
    "where is a parameter, 0   1, called the discount rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
