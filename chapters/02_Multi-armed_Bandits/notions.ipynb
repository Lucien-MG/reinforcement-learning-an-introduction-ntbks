{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits\n",
    "\n",
    "The most important feature distinguishing reinforcement learning from other types of  \n",
    "learning is that it uses training information that evaluates the actions taken rather  \n",
    "than instructs by giving correct actions.\n",
    "\n",
    "There's multiple type of feedbacks:\n",
    "- Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible.\n",
    "- Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken.\n",
    "\n",
    "In this chapter we study the evaluative aspect of reinforcement learning in a simplified  \n",
    "setting, one that does not involve learning to act in more than one situation. This  \n",
    "nonassociative setting is the one in which most prior work involving evaluative feedback  \n",
    "has been done, and it avoids much of the complexity of the full reinforcement learning  \n",
    "problem.\n",
    "\n",
    "The particular nonassociative, evaluative feedback problem that we explore is a simple  \n",
    "version of the k-armed bandit problem.\n",
    "\n",
    "## 2.1 A k-armed Bandit Problem\n",
    "\n",
    "### What is k-armed bandit problem\n",
    "\n",
    "Consider the following learning problem. You are faced repeatedly with a choice among  \n",
    "k different options, or actions. After each choice you receive a numerical reward chosen  \n",
    "from a stationary probability distribution that depends on the action you selected.  \n",
    "Your objective is to maximize the expected total reward over some time period, for example,  \n",
    "over 1000 action selections, or time steps.\n",
    "\n",
    "In our **k-armed bandit problem**, each of the k actions has an expected or mean reward  \n",
    "given that that action is selected; let us call this the **value** of that action.  \n",
    "We denote the action selected on time step t as $A_t$, and the corresponding reward as $R_t$.  \n",
    "The value then of an arbitrary action $a$, denoted $q_*(a)$, is the expected reward given that a is selected:\n",
    "\n",
    "$q_*(a) = E[R_t | A_t = a]$\n",
    "\n",
    "If you knew the value of each action, then it would be trivial to solve the k-armed bandit  \n",
    "problem: you would always select the action with highest value. We assume that you do  \n",
    "not know the action values with certainty, although you may have estimates. We denote  \n",
    "the estimated value of action $a$ at time step $t$ as $Q_t(a)$.\n",
    "We would like $Q_t(a)$ to be close to $q_*(a)$.\n",
    "\n",
    "If you maintain estimates of the **action values**, then at any time step there is at least  \n",
    "one action whose estimated value is greatest. We call these the **greedy actions**. When you  \n",
    "select one of these actions, we say that you are **exploiting** your current knowledge of the  \n",
    "values of the actions. If instead you select one of the **nongreedy actions**, then we say you  \n",
    "are **exploring**, because this enables you to improve your estimate of the nongreedy action’s  \n",
    "value\n",
    "\n",
    "### K-armed bandit problem: Implementation\n",
    "\n",
    "#### Numpy\n",
    "\n",
    "[NumPy](https://numpy.org/) is the fundamental package for scientific computing in Python.  \n",
    "It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices),  \n",
    "and an assortment of routines for fast operations on arrays, including mathematical, logical,  \n",
    "shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\n",
    "\n",
    "#### Gymnasium\n",
    "\n",
    "To create our environement k-armed bandit, we're gonne use gym,  \n",
    "which is kind of the standard to create reproductible reinforcement learning environments.\n",
    "\n",
    "[Gym](https://www.gymlibrary.dev/index.html) is an open source Python library for developing and comparing reinforcement learning algorithms  \n",
    "by providing a standard API to communicate between learning algorithms and environments,  \n",
    "as well as a standard set of environments compliant with that API.  \n",
    "Since its release, Gym's API has become the field standard for doing this.\n",
    "\n",
    "[Gymnasium](https://gymnasium.farama.org/) is a maintained fork of OpenAI’s Gym library.  \n",
    "The Gymnasium interface is simple, pythonic, and capable of representing general RL problems,  \n",
    "and has a compatibility wrapper for old Gym environments.\n",
    "\n",
    "### K-armed Bandit Gymnasium Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from numba import int32, float64\n",
    "from numba.experimental import jitclass\n",
    "\n",
    "class KArmedBandit(gym.Env):\n",
    "\n",
    "    def __init__(self, nb_arms=10, nb_steps=1000, mean=0, variance=1, noise_variance=1):\n",
    "        self._nb_arms = nb_arms\n",
    "        self._nb_steps = nb_steps\n",
    "\n",
    "        self._mean = 0\n",
    "        self._noise_mean = 0\n",
    "        self._variance = variance\n",
    "        self._noise_variance = noise_variance\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(nb_arms)\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._step += 1\n",
    "    \n",
    "        reward = self._arms[action]\n",
    "        reward_noise = self.np_random.normal(self._noise_mean, self._noise_variance)\n",
    "        terminated = self._step >= self._nb_steps\n",
    "\n",
    "        info = { \"is_optimal_action\": int(action == np.argmax(self._arms)) }\n",
    "        reward = reward + reward_noise\n",
    "\n",
    "        return None, reward, terminated, False, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self._step = 0\n",
    "        self._arms = self.np_random.normal(self._mean, self._variance, size=self._nb_arms)\n",
    "\n",
    "        return None, {}\n",
    "\n",
    "plop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Action-value Methods\n",
    "\n",
    "We begin by looking more closely at methods for estimating the values of actions and  \n",
    "for using the estimates to make action selection decisions, which we collectively call **action-value methods**.  \n",
    "Recall that the true value of an action is the mean reward when that action is selected.  \n",
    "One natural way to estimate this is by averaging the rewards actually received:  \n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "Q_t(a) &= {sum-of-rewards-when-a-taken-prior-to-t \\over number-of-times-a-taken-prior-to-t} \\\\\n",
    "       &= {\\sum_{i=1}^{t-1} R_i · 1_{A_i = a} \\over \\sum_{i=1}^{t-1} 1_{A_i=a}}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "where predicate denotes the random variable that is 1 if predicate is true and 0 if it is not.  \n",
    "If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as 0.  \n",
    "As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to  \n",
    "$q_*(a)$. We call this the sample-average method for estimating action values because each  \n",
    "estimate is an average of the sample of relevant rewards.\n",
    "\n",
    "The simplest action selection rule is to select one of the actions with the highest  \n",
    "estimated value, that is, one of the greedy actions as defined in the previous section.  \n",
    "If there is more than one greedy action, then a selection is made among them in some  \n",
    "arbitrary way, perhaps randomly. We write this greedy action selection method as  \n",
    "\n",
    "$\n",
    "A_{t} .= argmax_a Q_t(a)\n",
    "$ \n",
    "\n",
    "where argmax a denotes the action a for which the expression that follows is maximized (with ties broken arbitrarily).  \n",
    "Greedy action selection always exploits current knowledge to maximize immediate reward;  \n",
    "it spends no time at all sampling apparently inferior actions to see if they might really be better.  \n",
    "A simple alternative is to behave greedily most of the time, but every once in a while,  \n",
    "say with small probability $\\epsilon$, instead select randomly from among all the actions with equal probability,  \n",
    "independently of the action-value estimates.\n",
    "\n",
    "We call methods using this near-greedy action selection rule **$\\epsilon$-greedy methods**.  \n",
    "An advantage of these methods is that, in the limit as the number of steps increases,  \n",
    "every action will be sampled an infinite number of times, thus ensuring that all the $Q_t(a)$ converge to $q_*(a)$.  \n",
    "This of course implies that the probability of selecting the optimal action converges to greater than $1-\\epsilon$, that is, to near certainty.  \n",
    "\n",
    "These are just asymptotic guarantees, however, and say little about the practical effectiveness of the methods.\n",
    "\n",
    "#### Epsilon Greedy: Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import int32, float64\n",
    "from numba.experimental import jitclass\n",
    "\n",
    "# Here we use numba to accelerate our code\n",
    "# You can just ignore the jitclass part it doesn't impact the code logic\n",
    "@jitclass([\n",
    "    ('nb_actions', int32),\n",
    "    ('epsilon', float64),\n",
    "    ('q', float64[:]),\n",
    "    ('reward_sum', float64[:]),\n",
    "    ('nb_action_taken', float64[:]),\n",
    "])\n",
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.reward_sum = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.zeros(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken[action] += 1\n",
    "        self.reward_sum[action] += reward\n",
    "\n",
    "        self.q[action] = self.reward_sum[action] / self.nb_action_taken[action]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.reward_sum = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.zeros(self.nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The 10-armed Testbed\n",
    "\n",
    "To roughly assess the relative effectiveness of the greedy and $\\epsilon$-greedy action-value methods,  \n",
    "we compared them numerically on a suite of test problems.  \n",
    "This was a set of 2000 randomly generated k-armed bandit problems with $k=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../tools/armed_bandits.py\n",
    "\n",
    "nb_arms  = 10   # k = 10\n",
    "nb_steps = 2000 # Set of 2000 randomly generated k-armed bandit problems\n",
    "\n",
    "# Create the gym k-armed-bandit environement\n",
    "env = KArmedBandit(nb_arms=nb_arms, nb_steps=nb_steps)\n",
    "\n",
    "# Plot sample distribution\n",
    "plot_env_sample(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools to run experiments\n",
    "\n",
    "To be able to run experiments we will need three functions:\n",
    "\n",
    "1. The first function will handle the interactions between our env and our agents.  \n",
    "This function need to return the result of the run, so that we can visualize it later.\n",
    "\n",
    "1. The second function will wrap the run function to be able to easily run experiments.  \n",
    "This function need to return the mean of all the runs, so that we can visualize it later.\n",
    "\n",
    "1. The third function will help us visualize the performance of our agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average performance of $\\epsilon$-greedy action-value methods on the 10-armed testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_01 = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.01)\n",
    "mean_rewards_01, percent_optimal_action_01 = run_exp(nb_exps, env, agent_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1)\n",
    "mean_rewards_1, percent_optimal_action_1 = run_exp(nb_exps, env, agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_greedy = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.0)\n",
    "mean_rewards_0, percent_optimal_action_0 = run_exp(nb_exps, env, agent_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp_1 = {\n",
    "    'Egreedy 0.01': {\n",
    "        \"mean_reward\": mean_rewards_01,\n",
    "        \"optimal_action\": percent_optimal_action_01,\n",
    "        \"color\": \"red\"\n",
    "    },\n",
    "    'Egreedy 0.1': {\n",
    "        \"mean_reward\": mean_rewards_1,\n",
    "        \"optimal_action\": percent_optimal_action_1,\n",
    "        \"color\": \"blue\"\n",
    "    },\n",
    "    'Greedy 0.0': {\n",
    "        \"mean_reward\": mean_rewards_0,\n",
    "        \"optimal_action\": percent_optimal_action_0,\n",
    "        \"color\": \"green\"\n",
    "    },\n",
    "}\n",
    "\n",
    "plot_results(\n",
    "    \"Compares greedy method with different parameters (0.01, 0.1 and 0)\",\n",
    "    [\"Average Reward / Steps\", \"Optimal Action / Steps\"],\n",
    "    results_exp_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- Greedy Method: Initially improves faster but plateaus at a lower reward level (≈1), performing poorly long-term due to getting stuck on suboptimal actions. Finds the optimal action only about 33% of the time.\n",
    "\n",
    "- Epsilon-Greedy (ε=0.1): Explores more, finds the optimal action earlier, but never selects it more than 91% of the time.\n",
    "\n",
    "- Epsilon-Greedy (ε=0.01): Improves slower but eventually outperforms the ε=0.1 method in both reward and optimal action selection.\n",
    "General: Epsilon-greedy methods outperform the greedy method due to continued exploration. Reducing epsilon over time can combine the benefits of high and low epsilon values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action-value methods we have discussed so far all estimate action values as sample\n",
    "averages of observed rewards. We now turn to the question of how these averages can be\n",
    "computed in a computationally efficient manner, in particular, with constant memory and constant per-time-step computation.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_n &= {R_1 + R_2 + ... + R_{n-1} \\over n - 1} \\\\\n",
    "\\\\\n",
    "Q_{n + 1} &= {1 \\over n} \\sum{i=1}\\\\\n",
    "       &= {\\sum_{i=1}^{t-1} R_i · 1_{A_i = a} \\over \\sum_{i=1}^{t-1} 1_{A_i=a}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So, We can write Epsilon Greedy algorithm like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jitclass([\n",
    "    ('nb_actions', int32),\n",
    "    ('epsilon', float64),\n",
    "    ('q', float64[:]),\n",
    "    ('nb_action_taken', float64[:]),\n",
    "])\n",
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken[action] += 1\n",
    "        self.q[action] += (reward - self.q[action]) / self.nb_action_taken[action]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Tracking a Nonstationary Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 Optimistic Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jitclass([\n",
    "    ('nb_actions', int32),\n",
    "    ('epsilon', float64),\n",
    "    ('optimistic_value', float64),\n",
    "    ('q', float64[:]),\n",
    "    ('nb_times_taken_action', float64[:]),\n",
    "])\n",
    "class EpsilonGreedyOptimisticInitValues():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon=0.1, optimistic_value=0):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.optimistic_value = optimistic_value\n",
    "\n",
    "        self.nb_times_taken_action = np.zeros(self.nb_actions)\n",
    "        self.q = np.ones(self.nb_actions) * optimistic_value\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_times_taken_action[action] += 1\n",
    "        self.q[action] += 0.1 * (reward - self.q[action]) # / self.nb_times_taken_action[action]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.nb_times_taken_action = np.zeros(self.nb_actions)\n",
    "        self.q = np.ones(self.nb_actions) * self.optimistic_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_optimistic_greedy = EpsilonGreedyOptimisticInitValues(nb_actions=env.action_space.n, epsilon=0, optimistic_value=5)\n",
    "mean_rewards_optimistic, percent_optimal_action_optimistic = run_exp(nb_exps, env, agent_optimistic_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_non_optimistic = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1)\n",
    "mean_rewards_non_optimistic, percent_optimal_action_non_optimistic = run_exp(nb_exps, env, agent_non_optimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp_2 = {\n",
    "    'Greedy Optimistic': {\n",
    "        \"mean_reward\": mean_rewards_optimistic,\n",
    "        \"optimal_action\": percent_optimal_action_optimistic,\n",
    "        \"color\": \"blue\"\n",
    "    },\n",
    "    'Egreedy Non Optimistic': {\n",
    "        \"mean_reward\": mean_rewards_non_optimistic,\n",
    "        \"optimal_action\": percent_optimal_action_non_optimistic,\n",
    "        \"color\": \"red\"\n",
    "    }\n",
    "}\n",
    "\n",
    "plot_results(\"Optimistic greedy vs Non optimistic 0.01\", [\"Average Reward / Steps\", \"Optimal Action / Steps\"], results_exp_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper-Confidence-Bound Action Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import int32, float64\n",
    "from numba.experimental import jitclass\n",
    "\n",
    "@jitclass([\n",
    "    ('nb_actions', int32),\n",
    "    ('confidence', float64),\n",
    "    ('alpha', float64),\n",
    "    ('q', float64[:]),\n",
    "    ('nb_action_taken', float64[:]),\n",
    "    ('upper_configdence', float64[:]),\n",
    "])\n",
    "class UpperConfidenceBound():\n",
    "\n",
    "    def __init__(self, nb_actions, confidence, alpha=0.1):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.confidence = confidence\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.zeros(self.nb_actions)\n",
    "        self.upper_configdence = np.ones(self.nb_actions) * np.inf\n",
    "\n",
    "    def action(self):\n",
    "        return np.argmax(self.q + self.upper_configdence)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken[action] += 1\n",
    "\n",
    "        self.q[action] += (reward - self.q[action]) / self.nb_action_taken[action]\n",
    "\n",
    "        if not 0 in self.nb_action_taken:\n",
    "            self.upper_configdence = self.confidence * np.sqrt(np.log(np.sum(self.nb_action_taken)) / self.nb_action_taken)\n",
    "        else:\n",
    "            self.upper_configdence[action] = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.zeros(self.nb_actions)\n",
    "        self.upper_configdence = np.ones(self.nb_actions) * np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_condidence_agent = UpperConfidenceBound(nb_actions=env.action_space.n, confidence=2, alpha=0.1)\n",
    "mean_rewards_upper_confidence, percent_optimal_action_upper_confidence = run_exp(nb_exps, env, upper_condidence_agent)\n",
    "#1m8.1s vs 21s w numba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egreedy_agent = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1)\n",
    "mean_rewards_egreedy, percent_optimal_action_egreedy = run_exp(nb_exps, env, egreedy_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp_3 = {\n",
    "    'UCB': {\n",
    "        \"mean_reward\": mean_rewards_upper_confidence,\n",
    "        \"optimal_action\": percent_optimal_action_upper_confidence,\n",
    "        \"color\": \"blue\"\n",
    "    },\n",
    "    'Egreedy 0.1': {\n",
    "        \"mean_reward\": mean_rewards_egreedy,\n",
    "        \"optimal_action\": percent_optimal_action_egreedy,\n",
    "        \"color\": \"red\"\n",
    "    }\n",
    "}\n",
    "\n",
    "plot_results(\"Upper Confidence Bound vs Epsilon Greedy\", [\"Average Reward / Steps\", \"Optimal Action / Steps\"], results_exp_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8 Gradient Bandit Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jitclass([\n",
    "    ('nb_actions', int32),\n",
    "    ('alpha', float64),\n",
    "    ('q', float64[:]),\n",
    "    ('soft_probs', float64[:]),\n",
    "    ('mean_reward', float64),\n",
    "    ('nb_action_taken', float64),\n",
    "])\n",
    "class GradientBandit():\n",
    "\n",
    "    def __init__(self, nb_actions, alpha):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.alpha = alpha\n",
    "        self.soft_probs = np.zeros(nb_actions)\n",
    "        self.nb_action_taken = 0\n",
    "\n",
    "        self.mean_reward = 0\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "\n",
    "    def _softmax(self):\n",
    "        e_x = np.exp(self.q - np.max(self.q))\n",
    "        probs = e_x / e_x.sum(axis=0)\n",
    "        return probs\n",
    "    \n",
    "    def rand_choice_nb(self, prob):\n",
    "        return np.searchsorted(np.cumsum(prob), np.random.random(), side=\"right\")\n",
    "\n",
    "    def action(self):\n",
    "        self.soft_probs = self._softmax()\n",
    "        return self.rand_choice_nb(self.soft_probs)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken += 1\n",
    "        self.mean_reward += (reward - self.mean_reward) / self.nb_action_taken\n",
    "\n",
    "        self.soft_probs[action] = - (1 - self.soft_probs[action])\n",
    "        self.q -= self.alpha * (reward - self.mean_reward) * self.soft_probs\n",
    "    \n",
    "    def reset(self):\n",
    "        self.nb_action_taken = 0\n",
    "        self.q = np.zeros(self.nb_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_agent = GradientBandit(nb_actions=env.action_space.n, alpha=0.1)\n",
    "mean_rewards_gb, percent_optimal_action_gb = run_exp(nb_exps, env, gb_agent)\n",
    "#2m23s vs 23s w numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egreedy_agent = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1)\n",
    "mean_rewards_egreedy, percent_optimal_action_egreedy = run_exp(nb_exps, env, egreedy_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp_4 = {\n",
    "    'GradientBandit Alpha=0.1': {\n",
    "        \"mean_reward\": mean_rewards_gb,\n",
    "        \"optimal_action\": percent_optimal_action_gb,\n",
    "        \"color\": \"blue\"\n",
    "    },\n",
    "    'Egreedy 0.01': {\n",
    "        \"mean_reward\": mean_rewards_egreedy,\n",
    "        \"optimal_action\": percent_optimal_action_egreedy,\n",
    "        \"color\": \"green\"\n",
    "    }\n",
    "}\n",
    "\n",
    "plot_results(\"Gradient Bandit vs Epsilon Greedy\", [\"Average Reward / Steps\", \"Optimal Action / Steps\"], results_exp_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.9 Associative Search (Contextual Bandits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.10 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parameter_study(nb_exps, env, agents_parameters):\n",
    "    results_mean_reward = {}\n",
    "    results_percent_optimal_action = {}\n",
    "\n",
    "    for agent_name in agents_parameters:\n",
    "        results_mean_reward[agent_name] = []\n",
    "        results_percent_optimal_action[agent_name] = []\n",
    "\n",
    "    for agent_name in agents_parameters:\n",
    "        print(agent_name)\n",
    "\n",
    "        for parameter in agents_parameters[agent_name][\"parameters\"]:\n",
    "            print(\"    running parameters:\", parameter)\n",
    "\n",
    "            agent = agents_parameters[agent_name][\"class\"](nb_actions=env.action_space.n, **parameter)\n",
    "\n",
    "            mean_reward_over_steps, percent_optimal_action_over_steps = run_exp(nb_exps, env, agent)\n",
    "\n",
    "            mean_reward = np.mean(mean_reward_over_steps)\n",
    "            mean_optimal_action_percent = np.mean(percent_optimal_action_over_steps)\n",
    "\n",
    "            results_mean_reward[agent_name].append(mean_reward)\n",
    "            results_percent_optimal_action[agent_name].append(mean_optimal_action_percent)\n",
    "\n",
    "    return results_mean_reward, results_percent_optimal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_study_results(agents_parameters, results_mean_reward, results_percent_optimal_action):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Mean Reward / Parameters\", \"Mean Optimal Action / Parameters\"])\n",
    "\n",
    "    x = []\n",
    "\n",
    "    for agent_name in results_mean_reward:\n",
    "        parameter = agents_parameters[agent_name][\"variable\"]\n",
    "        x += [p[parameter] for p in agents_parameters[agent_name][\"parameters\"]]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[p[parameter] for p in agents_parameters[agent_name][\"parameters\"]],\n",
    "                       y=results_mean_reward[agent_name], line_color=agents_parameters[agent_name][\"color\"],\n",
    "                       name=agent_name)\n",
    "        , row=1, col=1)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[p[parameter] for p in agents_parameters[agent_name][\"parameters\"]],\n",
    "                       y=results_percent_optimal_action[agent_name], line_color=agents_parameters[agent_name][\"color\"],\n",
    "                       showlegend=False)\n",
    "        , row=1, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Parameter Study\",\n",
    "        legend_title=\"Parameters\",\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        type='category',\n",
    "        tickmode= 'array',\n",
    "        categoryorder= 'array',\n",
    "        categoryarray= sorted(x))\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = {\n",
    "    \"EpsilonGreedy\": {\n",
    "        \"class\": EpsilonGreedy,\n",
    "        \"color\": \"red\",\n",
    "        \"variable\": \"epsilon\",\n",
    "        \"parameters\": [\n",
    "            {\"epsilon\": 1 / 128},\n",
    "            {\"epsilon\": 1 / 64},\n",
    "            {\"epsilon\": 1 / 32},\n",
    "            {\"epsilon\": 1 / 16},\n",
    "            {\"epsilon\": 1 / 8},\n",
    "            {\"epsilon\": 1 / 4}\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"Greedy Optimistic\": {\n",
    "        \"class\": EpsilonGreedyOptimisticInitValues,\n",
    "        \"color\": \"black\",\n",
    "        \"variable\": \"optimistic_value\",\n",
    "        \"parameters\": [\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 1 / 4},\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 1 / 2},\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 1},\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 2},\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 4},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"UCB\": {\n",
    "        \"class\": UpperConfidenceBound,\n",
    "        \"color\": \"blue\",\n",
    "        \"variable\": \"confidence\",\n",
    "        \"parameters\": [\n",
    "            {\"confidence\": 1 / 16},\n",
    "            {\"confidence\": 1 / 8},\n",
    "            {\"confidence\": 1 / 4},\n",
    "            {\"confidence\": 1 / 2},\n",
    "            {\"confidence\": 1},\n",
    "            {\"confidence\": 2},\n",
    "            {\"confidence\": 4},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"Gradient Bandit\": {\n",
    "        \"class\": GradientBandit,\n",
    "        \"color\": \"green\",\n",
    "        \"variable\": \"alpha\",\n",
    "        \"parameters\": [\n",
    "            {\"alpha\": 1 / 32},\n",
    "            {\"alpha\": 1 / 16},\n",
    "            {\"alpha\": 1 / 8},\n",
    "            {\"alpha\": 1 / 4},\n",
    "            {\"alpha\": 1 / 2},\n",
    "            {\"alpha\": 1},\n",
    "            {\"alpha\": 2},\n",
    "        ],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000\n",
    "results_mean_reward, results_percent_optimal_action = run_parameter_study(nb_exps, env, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_study_results(agents, results_mean_reward, results_percent_optimal_action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
