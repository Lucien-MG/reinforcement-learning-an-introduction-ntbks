{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits\n",
    "\n",
    "The most important feature distinguishing reinforcement learning from other types of  \n",
    "learning is that it uses training information that evaluates the actions taken rather  \n",
    "than instructs by giving correct actions.\n",
    "\n",
    "There's multiple type of feedbacks:\n",
    "- Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible.\n",
    "- Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken.\n",
    "\n",
    "In this chapter we study the evaluative aspect of reinforcement learning in a simplified  \n",
    "setting, one that does not involve learning to act in more than one situation. This  \n",
    "nonassociative setting is the one in which most prior work involving evaluative feedback  \n",
    "has been done, and it avoids much of the complexity of the full reinforcement learning  \n",
    "problem.\n",
    "\n",
    "The particular nonassociative, evaluative feedback problem that we explore is a simple  \n",
    "version of the k-armed bandit problem.\n",
    "\n",
    "## A k-armed Bandit Problem\n",
    "\n",
    "Consider the following learning problem. You are faced repeatedly with a choice among  \n",
    "k different options, or actions. After each choice you receive a numerical reward chosen  \n",
    "from a stationary probability distribution that depends on the action you selected.  \n",
    "Your objective is to maximize the expected total reward over some time period, for example,  \n",
    "over 1000 action selections, or time steps.\n",
    "\n",
    "Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers.\n",
    "\n",
    "In our k-armed bandit problem, each of the k actions has an expected or mean reward  \n",
    "given that that action is selected; let us call this the value of that action. We denote the   \n",
    "action selected on time step t as $A_t$ , and the corresponding reward as $R_t$.  \n",
    "The value then of an arbitrary action $a$, denoted $q_*(a)$, is the expected reward given that a is selected:\n",
    "\n",
    "$q_*(a) = E[R_t | A_t = a]$\n",
    "\n",
    "If you knew the value of each action, then it would be trivial to solve the k-armed bandit  \n",
    "problem: you would always select the action with highest value. We assume that you do  \n",
    "not know the action values with certainty, although you may have estimates. We denote  \n",
    "the estimated value of action $a$ at time step $t$ as $Q_t(a)$.\n",
    "We would like $Q_t(a)$ to be close to $q_*(a)$.\n",
    "\n",
    "If you maintain estimates of the action values, then at any time step there is at least  \n",
    "one action whose estimated value is greatest. We call these the greedy actions. When you  \n",
    "select one of these actions, we say that you are exploiting your current knowledge of the  \n",
    "values of the actions. If instead you select one of the nongreedy actions, then we say you  \n",
    "are exploring, because this enables you to improve your estimate of the nongreedy action’s  \n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-armed bandit problem implementation\n",
    "\n",
    "#### Numpy\n",
    "\n",
    "For calculation, we're going to use numpy.\n",
    "\n",
    "#### Gymnasium\n",
    "\n",
    "To create our environement k-armed bandit, we're gonne use gym,  \n",
    "which is kind of the standard to create reproductible reinforcement learning environments.\n",
    "\n",
    "Gym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym's API has become the field standard for doing this.\n",
    "\n",
    "Gymnasium is a maintained fork of OpenAI’s Gym library. The Gymnasium interface is simple, pythonic, and capable of representing general RL problems, and has a compatibility wrapper for old Gym environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBandit(gym.Env):\n",
    "\n",
    "    def __init__(self, nb_arms=10, nb_steps=1000, mean=0, variance=1, noise_variance=1):\n",
    "        self._nb_arms = nb_arms\n",
    "        self._nb_steps = nb_steps\n",
    "\n",
    "        self._mean = 0\n",
    "        self._noise_mean = 0\n",
    "        self._variance = variance\n",
    "        self._noise_variance = noise_variance\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(nb_arms)\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._step += 1\n",
    "    \n",
    "        reward = self._arms[action]\n",
    "        reward_noise = self.np_random.normal(self._noise_mean, self._noise_variance)\n",
    "        terminated = self._step >= self._nb_steps\n",
    "\n",
    "        info = { \"is_optimal_action\": int(action == np.argmax(self._arms)) }\n",
    "\n",
    "        return reward + reward_noise, terminated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self._step = 0\n",
    "        self._arms = self.np_random.normal(self._mean, self._variance, size=self._nb_arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(main_title, titles, results):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=titles)\n",
    "\n",
    "    for result_name in results:\n",
    "        x = np.arange(len(results[result_name][\"mean_reward\"]))\n",
    "        fig.add_trace(go.Scatter(x=x, y=results[result_name][\"mean_reward\"], line_color=results[result_name][\"color\"], name=result_name), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=x, y=results[result_name][\"optimal_action\"], line_color=results[result_name][\"color\"], name=result_name, showlegend=False), row=1, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=main_title,\n",
    "        legend_title=\"Parameters\",\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-value Methods\n",
    "\n",
    "We begin by looking more closely at methods for estimating the values of actions and\n",
    "for using the estimates to make action selection decisions, which we collectively call\n",
    "action-value methods. Recall that the true value of an action is the mean reward when\n",
    "that action is selected. One natural way to estimate this is by averaging the rewards\n",
    "actually received:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_t(a) &= {sum-of-rewards-when-a-taken-prior-to-t \\over number-of-times-a-taken-prior-to-t} \\\\\n",
    "       &= {\\sum_{i=1}^{t-1} R_i · 1_{A_i = a} \\over \\sum_{i=1}^{t-1} 1_{A_i=a}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where predicate denotes the random variable that is 1 if predicate is true and 0 if it is not.\n",
    "If the denominator is zero, then we instead define Q t ( a ) as some default value, such as\n",
    "0. As the denominator goes to infinity, by the law of large numbers, Q t (a) converges to\n",
    "q ⇤ ( a ). We call this the sample-average method for estimating action values because each\n",
    "estimate is an average of the sample of relevant rewards\n",
    "\n",
    "he simplest action selection rule is to select one of the actions with the highest\n",
    "estimated value, that is, one of the greedy actions as defined in the previous section.\n",
    "If there is more than one greedy action, then a selection is made among them in some\n",
    "arbitrary way, perhaps randomly. We write this greedy action selection method as\n",
    "A t\n",
    ".\n",
    "= argmax\n",
    "a\n",
    "Q t (a), \n",
    "\n",
    "where argmax a denotes the action a for which the expression that follows is maximized\n",
    "(again, with ties broken arbitrarily). Greedy action selection always exploits current\n",
    "knowledge to maximize immediate reward; it spends no time at all sampling apparently\n",
    "inferior actions to see if they might really be better. A simple alternative is to behave\n",
    "greedily most of the time, but every once in a while, say with small probability \" , instead\n",
    "28 Chapter 2: Multi-armed Bandits\n",
    "select randomly from among all the actions with equal probability, independently of\n",
    "the action-value estimates. We call methods using this near-greedy action selection rule\n",
    "\" -greedy methods. An advantage of these methods is that, in the limit as the number of\n",
    "steps increases, every action will be sampled an infinite number of times, thus ensuring\n",
    "that all the Q t ( a ) converge to q ⇤ ( a ). This of course implies that the probability of selecting\n",
    "the optimal action converges to greater than 1  \" , that is, to near certainty. These are\n",
    "just asymptotic guarantees, however, and say little about the practical e↵ectiveness of\n",
    "the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action-value Method implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.reward_sum = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.zeros(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken[action] += 1\n",
    "        self.reward_sum[action] += reward\n",
    "\n",
    "        self.q[action] = self.reward_sum[action] / self.nb_action_taken[action]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.reward_sum = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.zeros(self.nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 10-armed Testbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To roughly assess the relative e↵ectiveness of the greedy and \" -greedy action-value\n",
    "methods, we compared them numerically on a suite of test problems. This was a set\n",
    "of 2000 randomly generated k -armed bandit problems with k = 10. For each bandit\n",
    "problem, such as the one shown in Figure 2.1, the action values, q ⇤ ( a ), a = 1 , . . . , 10,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_arms  = 10\n",
    "nb_steps = 2000\n",
    "\n",
    "env = KArmedBandit(nb_arms=nb_arms, nb_steps=nb_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example bandit problem from the 10-armed testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# Sample our distribution to see it's correct\n",
    "data = np.array([[env.step(i)[0] for _ in range(2000)] for i in range(len(env._arms))])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(env._arms)):\n",
    "    fig.add_trace(go.Violin(x=[i] * len(data[i]), y=data[i], name=\"q*(\" + str(i) + \") = \" + str(env._arms[i])[:4], meanline_visible=True))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"K-Armed Bandit Problem Distribution\",\n",
    "    xaxis_title=\"Actions\",\n",
    "    yaxis_title=\"Reward Distributions\",\n",
    "    legend_title=\"True Value of q*(a)\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(env, agent):\n",
    "    list_of_reward = []\n",
    "    list_of_optimal_action = []\n",
    "\n",
    "    env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    terminated = False\n",
    "\n",
    "    while not terminated:\n",
    "        action = agent.action()\n",
    "\n",
    "        reward, terminated, info = env.step(action)\n",
    "\n",
    "        agent.observe(action, reward)\n",
    "\n",
    "        list_of_reward.append(reward)\n",
    "        list_of_optimal_action.append(info[\"is_optimal_action\"])\n",
    "    \n",
    "    return np.array(list_of_reward), np.array(list_of_optimal_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(nb_exps, env, agent):\n",
    "    list_rewards, list_optimal_action = run_env(env, agent)\n",
    "\n",
    "    for _ in range(nb_exps - 1):\n",
    "        list_rewards_tmp, list_optimal_action_tmp = run_env(env, agent)\n",
    "\n",
    "        list_rewards += list_rewards_tmp\n",
    "        list_optimal_action += list_optimal_action_tmp\n",
    "\n",
    "    return list_rewards / nb_exps, (list_optimal_action / nb_exps) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_01 = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.01)\n",
    "mean_rewards_01, percent_optimal_action_01 = run_exp(nb_exps, env, agent_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1)\n",
    "mean_rewards_1, percent_optimal_action_1 = run_exp(nb_exps, env, agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_greedy = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.0)\n",
    "mean_rewards_0, percent_optimal_action_0 = run_exp(nb_exps, env, agent_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp_1 = {\n",
    "    'Egreedy 0.01': {\n",
    "        \"mean_reward\": mean_rewards_01,\n",
    "        \"optimal_action\": percent_optimal_action_01,\n",
    "        \"color\": \"red\"\n",
    "    },\n",
    "    'Egreedy 0.1': {\n",
    "        \"mean_reward\": mean_rewards_1,\n",
    "        \"optimal_action\": percent_optimal_action_1,\n",
    "        \"color\": \"blue\"\n",
    "    },\n",
    "    'Greedy 0.0': {\n",
    "        \"mean_reward\": mean_rewards_0,\n",
    "        \"optimal_action\": percent_optimal_action_0,\n",
    "        \"color\": \"green\"\n",
    "    },\n",
    "}\n",
    "\n",
    "plot_results(\n",
    "    \"Compares greedy method with different parameters (0.01, 0.1 and 0)\",\n",
    "    [\"Average Reward / Steps\", \"Optimal Action / Steps\"],\n",
    "    results_exp_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken[action] += 1\n",
    "        self.q[action] += (reward - self.q[action]) / self.nb_action_taken[action]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Tracking a Nonstationary Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 Optimistic Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyInitValue():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon=0.1, optimistic_value=0):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.optimistic_value = optimistic_value\n",
    "\n",
    "        self.nb_times_taken_action = np.zeros(self.nb_actions)\n",
    "        self.q = np.ones(self.nb_actions) * optimistic_value\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_times_taken_action[action] += 1\n",
    "        self.q[action] += 0.1 * (reward - self.q[action]) # / self.nb_times_taken_action[action]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.nb_times_taken_action = np.zeros(self.nb_actions)\n",
    "        self.q = np.ones(self.nb_actions) * self.optimistic_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7 Upper-Confidence-Bound Action Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperConfidenceBound():\n",
    "\n",
    "    def __init__(self, nb_actions, confidence, alpha=0.1):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.confidence = confidence\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.zeros(self.nb_actions)\n",
    "        self.upper_configdence = np.ones(self.nb_actions) * np.inf\n",
    "\n",
    "    def action(self):\n",
    "        return np.argmax(self.q + self.upper_configdence)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken[action] += 1\n",
    "\n",
    "        self.q[action] += (reward - self.q[action]) / self.nb_action_taken[action]\n",
    "\n",
    "        if not 0 in self.nb_action_taken:\n",
    "            self.upper_configdence = self.confidence * np.sqrt(np.log(np.sum(self.nb_action_taken)) / self.nb_action_taken)\n",
    "        else:\n",
    "            self.upper_configdence[action] = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.zeros(self.nb_actions)\n",
    "        self.upper_configdence = np.ones(self.nb_actions) * np.inf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8 Gradient Bandit Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(state_action_value):\n",
    "    e_x = np.exp(state_action_value - np.max(state_action_value))\n",
    "    probs = e_x / e_x.sum(axis=0)\n",
    "    return probs\n",
    "\n",
    "class GradientBandit():\n",
    "\n",
    "    def __init__(self, nb_actions, alpha):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.alpha = alpha\n",
    "        self.soft_probs = None\n",
    "        self.nb_action_taken = 0\n",
    "\n",
    "        self.mean_reward = 0\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        self.soft_probs = softmax(self.q)\n",
    "        return np.random.choice(self.nb_actions, 1, p=self.soft_probs)[0]\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken += 1\n",
    "        self.mean_reward += (reward - self.mean_reward) / self.nb_action_taken\n",
    "\n",
    "        self.soft_probs[action] = - (1 - self.soft_probs[action])\n",
    "        self.q -= self.alpha * (reward - self.mean_reward) * self.soft_probs\n",
    "    \n",
    "    def reset(self):\n",
    "        self.nb_action_taken = 0\n",
    "        self.q = np.zeros(self.nb_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.9 Associative Search (Contextual Bandits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.10 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parameter_study(nb_exps, env, agents_parameters):\n",
    "    results_mean_reward = {}\n",
    "    results_percent_optimal_action = {}\n",
    "\n",
    "    for agent_name in agents_parameters:\n",
    "        results_mean_reward[agent_name] = []\n",
    "        results_percent_optimal_action[agent_name] = []\n",
    "\n",
    "    for agent_name in agents_parameters:\n",
    "        print(agent_name)\n",
    "\n",
    "        for parameter in agents_parameters[agent_name][\"parameters\"]:\n",
    "            print(\"    running parameters:\", parameter)\n",
    "\n",
    "            agent = agents_parameters[agent_name][\"class\"](nb_actions=env.action_space.n, **parameter)\n",
    "\n",
    "            mean_reward_over_steps, percent_optimal_action_over_steps = run_exp(nb_exps, env, agent)\n",
    "\n",
    "            mean_reward = np.mean(mean_reward_over_steps)\n",
    "            mean_optimal_action_percent = np.mean(percent_optimal_action_over_steps)\n",
    "\n",
    "            results_mean_reward[agent_name].append(mean_reward)\n",
    "            results_percent_optimal_action[agent_name].append(mean_optimal_action_percent)\n",
    "\n",
    "    return results_mean_reward, results_percent_optimal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_study_results(agents_parameters, results_mean_reward, results_percent_optimal_action):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Mean Reward / Parameters\", \"Mean Optimal Action / Parameters\"])\n",
    "\n",
    "    x = []\n",
    "\n",
    "    for agent_name in results_mean_reward:\n",
    "        parameter = agents_parameters[agent_name][\"variable\"]\n",
    "        x += [p[parameter] for p in agents_parameters[agent_name][\"parameters\"]]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[p[parameter] for p in agents_parameters[agent_name][\"parameters\"]],\n",
    "                       y=results_mean_reward[agent_name], line_color=agents_parameters[agent_name][\"color\"],\n",
    "                       name=agent_name)\n",
    "        , row=1, col=1)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[p[parameter] for p in agents_parameters[agent_name][\"parameters\"]],\n",
    "                       y=results_percent_optimal_action[agent_name], line_color=agents_parameters[agent_name][\"color\"],\n",
    "                       showlegend=False)\n",
    "        , row=1, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Parameter Study\",\n",
    "        legend_title=\"Parameters\",\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        type='category',\n",
    "        tickmode= 'array',\n",
    "        categoryorder= 'array',\n",
    "        categoryarray= sorted(x))\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = {\n",
    "    \"EpsilonGreedy\": {\n",
    "        \"class\": EpsilonGreedy,\n",
    "        \"color\": \"red\",\n",
    "        \"variable\": \"epsilon\",\n",
    "        \"parameters\": [\n",
    "            {\"epsilon\": 1 / 128},\n",
    "            {\"epsilon\": 1 / 64},\n",
    "            {\"epsilon\": 1 / 32},\n",
    "            {\"epsilon\": 1 / 16},\n",
    "            {\"epsilon\": 1 / 8},\n",
    "            {\"epsilon\": 1 / 4}\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"Greedy Optimistic\": {\n",
    "        \"class\": EpsilonGreedy,\n",
    "        \"color\": \"black\",\n",
    "        \"variable\": \"optimistic_value\",\n",
    "        \"parameters\": [\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 1 / 4},\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 1 / 2},\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 1},\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 2},\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 4},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"UCB\": {\n",
    "        \"class\": UpperConfidenceBound,\n",
    "        \"color\": \"blue\",\n",
    "        \"variable\": \"confidence\",\n",
    "        \"parameters\": [\n",
    "            {\"confidence\": 1 / 16},\n",
    "            {\"confidence\": 1 / 8},\n",
    "            {\"confidence\": 1 / 4},\n",
    "            {\"confidence\": 1 / 2},\n",
    "            {\"confidence\": 1},\n",
    "            {\"confidence\": 2},\n",
    "            {\"confidence\": 4},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"Gradient Bandit\": {\n",
    "        \"class\": GradientBandit,\n",
    "        \"color\": \"green\",\n",
    "        \"variable\": \"alpha\",\n",
    "        \"parameters\": [\n",
    "            {\"alpha\": 1 / 32},\n",
    "            {\"alpha\": 1 / 16},\n",
    "            {\"alpha\": 1 / 8},\n",
    "            {\"alpha\": 1 / 4},\n",
    "            {\"alpha\": 1 / 2},\n",
    "            {\"alpha\": 1},\n",
    "            {\"alpha\": 2},\n",
    "        ],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000\n",
    "results_mean_reward, results_percent_optimal_action = run_parameter_study(nb_exps, env, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_study_results(agents, results_mean_reward, results_percent_optimal_action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
