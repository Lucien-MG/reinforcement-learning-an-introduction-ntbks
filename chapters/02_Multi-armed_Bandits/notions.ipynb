{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits\n",
    "\n",
    "The most important feature distinguishing reinforcement learning from other types of  \n",
    "learning is that it uses training information that evaluates the actions taken rather  \n",
    "than instructs by giving correct actions.\n",
    "\n",
    "There's multiple type of feedbacks:\n",
    "- Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible.\n",
    "- Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken.\n",
    "\n",
    "In this chapter we study the evaluative aspect of reinforcement learning in a simplified  \n",
    "setting, one that does not involve learning to act in more than one situation. This  \n",
    "nonassociative setting is the one in which most prior work involving evaluative feedback  \n",
    "has been done, and it avoids much of the complexity of the full reinforcement learning  \n",
    "problem.\n",
    "\n",
    "The particular nonassociative, evaluative feedback problem that we explore is a simple  \n",
    "version of the k-armed bandit problem.\n",
    "\n",
    "## A k-armed Bandit Problem\n",
    "\n",
    "Consider the following learning problem. You are faced repeatedly with a choice among  \n",
    "k different options, or actions. After each choice you receive a numerical reward chosen  \n",
    "from a stationary probability distribution that depends on the action you selected.  \n",
    "Your objective is to maximize the expected total reward over some time period, for example,  \n",
    "over 1000 action selections, or time steps.\n",
    "\n",
    "Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers.\n",
    "\n",
    "In our k-armed bandit problem, each of the k actions has an expected or mean reward  \n",
    "given that that action is selected; let us call this the value of that action. We denote the   \n",
    "action selected on time step t as $A_t$ , and the corresponding reward as $R_t$.  \n",
    "The value then of an arbitrary action $a$, denoted q ⇤ (a), is the expected reward given that a is selected:\n",
    "\n",
    "$q_*(a) = E[R_t | A_t = a]$\n",
    "\n",
    "If you knew the value of each action, then it would be trivial to solve the k -armed bandit\n",
    "problem: you would always select the action with highest value. We assume that you do\n",
    "not know the action values with certainty, although you may have estimates. We denote\n",
    "the estimated value of action a at time step t as Q t ( a ). We would like Q t ( a ) to be close\n",
    "to q ⇤ (a).\n",
    "\n",
    "If you maintain estimates of the action values, then at any time step there is at least\n",
    "one action whose estimated value is greatest. We call these the greedy actions. When you\n",
    "select one of these actions, we say that you are exploiting your current knowledge of the\n",
    "values of the actions. If instead you select one of the nongreedy actions, then we say you\n",
    "are exploring, because this enables you to improve your estimate of the nongreedy action’s\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-armed bandit problem implementation\n",
    "\n",
    "#### Numpy\n",
    "\n",
    "For calculation, we're going to use numpy.\n",
    "\n",
    "#### Gymnasium\n",
    "\n",
    "To create our environement k-armed bandit, we're gonne use gym.\n",
    "\n",
    "Gym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym's API has become the field standard for doing this.\n",
    "\n",
    "Gymnasium is a maintained fork of OpenAI’s Gym library. The Gymnasium interface is simple, pythonic, and capable of representing general RL problems, and has a compatibility wrapper for old Gym environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import plotly as plot\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBandit(gym.Env):\n",
    "\n",
    "    def __init__(self, nb_arms=10, nb_steps=1000, mean=0, variance=1, noise_variance=1):\n",
    "        self._nb_arms = nb_arms\n",
    "        self._nb_steps = nb_steps\n",
    "\n",
    "        self._mean = 0\n",
    "        self._noise_mean = 0\n",
    "        self._variance = variance\n",
    "        self._noise_variance = noise_variance\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(nb_arms)\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._step += 1\n",
    "    \n",
    "        reward = self._arms[action]\n",
    "        reward_noise = self.np_random.normal(self._noise_mean, self._noise_variance)\n",
    "        terminated = self._step >= self._nb_steps\n",
    "\n",
    "        info = { \"is_optimal_action\": int(action == np.argmax(self._arms)) }\n",
    "\n",
    "        return reward + reward_noise, terminated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self._step = 0\n",
    "        self._arms = self.np_random.normal(self._mean, self._variance, size=self._nb_arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(main_title, titles, results):\n",
    "    fig = plot.subplots.make_subplots(rows=1, cols=2, subplot_titles=titles)\n",
    "\n",
    "    for result_name in results:\n",
    "        x = np.arange(len(results[result_name][\"mean_reward\"]))\n",
    "        fig.add_trace(go.Scatter(x=x, y=results[result_name][\"mean_reward\"], line_color=results[result_name][\"color\"], name=result_name), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=x, y=results[result_name][\"optimal_action\"], line_color=results[result_name][\"color\"], name=result_name, showlegend=False), row=1, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=main_title,\n",
    "        legend_title=\"Parameters\",\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-value Methods\n",
    "\n",
    "We begin by looking more closely at methods for estimating the values of actions and\n",
    "for using the estimates to make action selection decisions, which we collectively call\n",
    "action-value methods. Recall that the true value of an action is the mean reward when\n",
    "that action is selected. One natural way to estimate this is by averaging the rewards\n",
    "actually received:\n",
    "\n",
    "Q t (a) .\n",
    "= sum of rewards when a taken prior to t\n",
    "number of times a taken prior to t =\n",
    "P t1\n",
    "i=1 R i · A i =a\n",
    "P t1\n",
    "i=1 A i =a\n",
    "\n",
    "where predicate denotes the random variable that is 1 if predicate is true and 0 if it is not.\n",
    "If the denominator is zero, then we instead define Q t ( a ) as some default value, such as\n",
    "0. As the denominator goes to infinity, by the law of large numbers, Q t (a) converges to\n",
    "q ⇤ ( a ). We call this the sample-average method for estimating action values because each\n",
    "estimate is an average of the sample of relevant rewards\n",
    "\n",
    "he simplest action selection rule is to select one of the actions with the highest\n",
    "estimated value, that is, one of the greedy actions as defined in the previous section.\n",
    "If there is more than one greedy action, then a selection is made among them in some\n",
    "arbitrary way, perhaps randomly. We write this greedy action selection method as\n",
    "A t\n",
    ".\n",
    "= argmax\n",
    "a\n",
    "Q t (a), \n",
    "\n",
    "where argmax a denotes the action a for which the expression that follows is maximized\n",
    "(again, with ties broken arbitrarily). Greedy action selection always exploits current\n",
    "knowledge to maximize immediate reward; it spends no time at all sampling apparently\n",
    "inferior actions to see if they might really be better. A simple alternative is to behave\n",
    "greedily most of the time, but every once in a while, say with small probability \" , instead\n",
    "28 Chapter 2: Multi-armed Bandits\n",
    "select randomly from among all the actions with equal probability, independently of\n",
    "the action-value estimates. We call methods using this near-greedy action selection rule\n",
    "\" -greedy methods. An advantage of these methods is that, in the limit as the number of\n",
    "steps increases, every action will be sampled an infinite number of times, thus ensuring\n",
    "that all the Q t ( a ) converge to q ⇤ ( a ). This of course implies that the probability of selecting\n",
    "the optimal action converges to greater than 1  \" , that is, to near certainty. These are\n",
    "just asymptotic guarantees, however, and say little about the practical e↵ectiveness of\n",
    "the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action-value Method implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 10-armed Testbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To roughly assess the relative e↵ectiveness of the greedy and \" -greedy action-value\n",
    "methods, we compared them numerically on a suite of test problems. This was a set\n",
    "of 2000 randomly generated k -armed bandit problems with k = 10. For each bandit\n",
    "problem, such as the one shown in Figure 2.1, the action values, q ⇤ ( a ), a = 1 , . . . , 10,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_arms  = 10\n",
    "nb_steps = 2000\n",
    "\n",
    "env = KArmedBandit(nb_arms=nb_arms, nb_steps=nb_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example bandit problem from the 10-armed testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# Sample our distribution to see it's correct\n",
    "data = np.array([[env.step(i)[0] for _ in range(2000)] for i in range(len(env._arms))])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(env._arms)):\n",
    "    fig.add_trace(go.Violin(x=[i] * len(data[i]), y=data[i], name=\"q*(\" + str(i) + \") = \" + str(env._arms[i])[:4], meanline_visible=True))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"K-Armed Bandit Problem Distribution\",\n",
    "    xaxis_title=\"Actions\",\n",
    "    yaxis_title=\"Reward Distributions\",\n",
    "    legend_title=\"True Value of q*(a)\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
