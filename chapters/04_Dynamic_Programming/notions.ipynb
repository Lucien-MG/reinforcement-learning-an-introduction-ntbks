{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "The term dynamic programming (DP) refers to a collection of algorithms that can be  \n",
    "used to compute optimal policies given a perfect model of the environment as a Markov  \n",
    "decision process (MDP).\n",
    "\n",
    "Classical DP algorithms are of limited utility in reinforcement  \n",
    "learning both because of their assumption of a perfect model and because of their great  \n",
    "computational expense, but they are still important theoretically.\n",
    "\n",
    "We usually assume that the environment is a finite MDP. That is, we assume that its  \n",
    "state, action, and reward sets, S, A, and R, are finite, and that its dynamics are given by a  \n",
    "set of probabilities p(s0 , r |s, a), for all s 2 S, a 2 A(s), r 2 R, and s0 2 S+ (S+ is S plus a  \n",
    "terminal state if the problem is episodic).\n",
    "\n",
    "The key idea of DP, and of reinforcement learning generally, is the use of value functions  \n",
    "to organize and structure the search for good policies.\n",
    "\n",
    "we can easily obtain optimal policies once we have found the optimal value functions, v⇤ or\n",
    "q⇤ , which satisfy the Bellman optimality equations:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "v_*(s) &= {max_a E[R_{t+1} + \\gamma * v_*(S_{t+1}) | S_t = s, A_t = a]} \\\\\n",
    "       &= {max_a \\sum_{s', r} p(s', r | s, a)[r + \\gamma *v_*(s') ]}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "q_*(s, a) &= {E[R_{t+1} + \\gamma * max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a]} \\\\\n",
    "          &= {\\sum_{s', r} p(s', r | s, a)[r + \\gamma * max_{a'} q_*(s', a') ]}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments,  \n",
    "that is, into update rules for improving approximations of the desired value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation (Prediction)\n",
    "\n",
    "First we consider how to compute the state-value function v⇡ for an arbitrary policy ⇡.\n",
    "This is called **policy evaluation** (or **prediction problem**) in the DP literature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
