{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "The term dynamic programming (DP) refers to a collection of algorithms that can be  \n",
    "used to compute optimal policies given a perfect model of the environment as a Markov  \n",
    "decision process (MDP).\n",
    "\n",
    "Classical DP algorithms are of limited utility in reinforcement  \n",
    "learning both because of their assumption of a perfect model and because of their great  \n",
    "computational expense, but they are still important theoretically.\n",
    "\n",
    "We usually assume that the environment is a finite MDP. That is, we assume that its  \n",
    "state, action, and reward sets, S, A, and R, are finite, and that its dynamics are given by a  \n",
    "set of probabilities p(s0 , r |s, a), for all s 2 S, a 2 A(s), r 2 R, and s0 2 S+ (S+ is S plus a  \n",
    "terminal state if the problem is episodic).\n",
    "\n",
    "The key idea of DP, and of reinforcement learning generally, is the use of value functions  \n",
    "to organize and structure the search for good policies.\n",
    "\n",
    "we can easily obtain optimal policies once we have found the optimal value functions, v⇤ or\n",
    "q⇤ , which satisfy the Bellman optimality equations:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "v_*(s) &= {max_a E[R_{t+1} + \\gamma * v_*(S_{t+1}) | S_t = s, A_t = a]} \\\\\n",
    "       &= {max_a \\sum_{s', r} p(s', r | s, a)[r + \\gamma *v_*(s') ]}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "q_*(s, a) &= {E[R_{t+1} + \\gamma * max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a]} \\\\\n",
    "          &= {\\sum_{s', r} p(s', r | s, a)[r + \\gamma * max_{a'} q_*(s', a') ]}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments,  \n",
    "that is, into update rules for improving approximations of the desired value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation (Prediction)\n",
    "\n",
    "First we consider how to compute the state-value function v⇡ for an arbitrary policy ⇡.\n",
    "This is called **policy evaluation** (or **prediction problem**) in the DP literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class PolicyIteration():\n",
    "\n",
    "    def __init__(self, action_space, theta):\n",
    "        self.theta = theta\n",
    "\n",
    "        self.V = defaultdict(lambda s: 0)\n",
    "        self.policy = defaultdict(lambda s: [1 / action_space for _ in len(action_space)])\n",
    "\n",
    "    def policy_evaluation(self, env):\n",
    "        delta = theta + 1\n",
    "\n",
    "        while delta > theta:\n",
    "\n",
    "    def policy_improvement(self, env):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jack's Car Rental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jack's Car Rental Env\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "class JacksCarRental(gym.Env):\n",
    "    metadata = { \"render_modes\": [\"human\", \"ascii\"] }\n",
    "\n",
    "    def __init__(self, render_mode=None, nb_cars_allowed=20):\n",
    "        self.nb_cars_allowed = nb_cars_allowed  # The maxium number of cars allowed\n",
    "        self._first_location = 0\n",
    "        self._second_location = 0\n",
    "\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"first_location\": spaces.Box(0, self.nb_cars_allowed, shape=(1,), dtype=int),\n",
    "                \"second_location\": spaces.Box(0, self.nb_cars_allowed, shape=(1,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self._agent_location = np.array([-1, -1], dtype=int)\n",
    "        self._target_location = np.array([-1, -1], dtype=int)\n",
    "\n",
    "        # We have 10 actions, corresponding to the number of cars moved (-5, -4, ..., 3, 4, 5)\n",
    "        self.action_space = spaces.Discrete(10)\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return { \"first_location\": self._first_location, \"second_location\": self._second_location}\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        cars_requested_at_first_location = self.np_random.poisson(3)\n",
    "        cars_returned_at_first_location = self.np_random.poisson(4)\n",
    "\n",
    "        cars_requested_at_second_location = self.np_random.poisson(3)\n",
    "        cars_returned_at_second_location = self.np_random.poisson(2)\n",
    "\n",
    "        nb_moved_cars = action\n",
    "\n",
    "        # We use `np.clip` to make sure we don't have an incorrect number of cars\n",
    "        self._first_location = np.clip(\n",
    "            self._first_location + nb_moved_cars + cars_returned_at_first_location, 0, 20\n",
    "        )\n",
    "\n",
    "        self._second_location = np.clip(\n",
    "            self._second_location - nb_moved_cars + cars_returned_at_second_location, 0, 20\n",
    "        )\n",
    "\n",
    "        reward_from_first_location = np.clip(self._first_location / cars_requested_at_first_location, 0, 1) * 10\n",
    "        reward_from_second_location = np.clip(self._first_location / cars_requested_at_first_location, 0, 1) * 10\n",
    "        reward_cost_from_moving_car = 2 * nb_moved_cars\n",
    "    \n",
    "        # An episode is done iff the agent has reached the target\n",
    "        terminated = self._step >= 1\n",
    "        reward = reward_from_first_location + reward_from_second_location - reward_cost_from_moving_car\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self._step = 0\n",
    "\n",
    "        self.first_location = 0\n",
    "        self.second_location = 0\n",
    "\n",
    "        return None, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_location': np.int64(3), 'second_location': np.int64(2)} 16.0 False False {}\n"
     ]
    }
   ],
   "source": [
    "env = JacksCarRental()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "observation, reward, termiated, truncated, info = env.step(2)\n",
    "print(observation, reward, termiated, truncated, info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
