{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "\n",
    "**Monte Carlo methods** require only experience sample sequences of states, actions, and rewards  \n",
    "from actual or simulated interaction with an environment. Learning from actual experience is striking  \n",
    "because it requires no prior knowledge of the environment’s dynamics, yet can still attain optimal behavior.\n",
    "\n",
    "Monte Carlo methods are ways of solving the reinforcement learning problem based on **averaging sample returns**.  \n",
    "To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks.  \n",
    "That is, we assume experience is divided into episodes, and that all episodes eventually terminate no matter  \n",
    "what actions are selected.\n",
    "\n",
    "Only on the completion of an episode are value estimates and policies changed.\n",
    "\n",
    "Monte Carlo methods can thus be incremental in an **episode-by-episode** sense, but not in a step-by-step (online) sense.  \n",
    "The term “Monte Carlo” is often used more broadly for any estimation method whose operation involves a **significant random component**.\n",
    "\n",
    "Here we use it specifically for methods based on averaging complete returns.\n",
    "\n",
    "There are multiple states, each acting like a different bandit problem (like an associative-search or contextual bandit)  \n",
    "and the different bandit problems are interrelated. That is, the return after taking an action in one state depends  \n",
    "on the actions taken in later states in the same episode. Because all the action selections are undergoing learning,  \n",
    "the problem becomes nonstationary from the point of view of the earlier state.\n",
    "\n",
    "To handle the nonstationarity, we adapt the idea of **general policy iteration** (GPI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Prediction\n",
    "\n",
    "Monte Carlo prediction methods allow to learn the state-value function for a given policy.  \n",
    "Recall that the value of a state is the expected return—expected cumulative future discounted reward—starting from that state.\n",
    "\n",
    "An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state.  \n",
    "As more returns are observed, the average should converge to the expected value. This idea underlies all Monte Carlo methods.\n",
    "\n",
    "The **first visit Monte Carlo method** estimates $v_\\pi(s)$ as the average of the returns following visits to $s$.  \n",
    "By the law of large numbers the sequence of averages of these estimates standard deviation of its error falls as $1/\\sqrt(n)$,\n",
    "where n is the number of returns averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class FirstVisitMonteCarloPrediction():\n",
    "\n",
    "    def __init__(self, gamma, policy):\n",
    "        self.gamma  = gamma\n",
    "        self.policy = policy\n",
    "\n",
    "        self.state_value = collections.defaultdict(lambda: 0)\n",
    "        self.returns = collections.defaultdict(lambda: 0)\n",
    "\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def action(self, state):\n",
    "        return self.policy(state)\n",
    "    \n",
    "    def observe(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def optimize(self):\n",
    "        g = 0\n",
    "\n",
    "        for t in reversed(range(len(self.states))):\n",
    "            g = self.gamma * g + self.rewards[t]\n",
    "\n",
    "            if not self.states[t] in self.states[0:t]:\n",
    "                self.returns[self.states[t]] += 1\n",
    "                self.state_value[self.states[t]] += (1 / self.returns[self.states[t]]) * (g - self.state_value[self.states[t]])\n",
    "        \n",
    "        self.states = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessaries libraries\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "import gymnasium as gym\n",
    "env = gym.make('Blackjack-v1', natural=True, sab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_env(env, agent):\n",
    "    terminated = False\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    while not terminated:\n",
    "        action = agent.action(observation)\n",
    "\n",
    "        new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        agent.observe(observation, action, reward)\n",
    "\n",
    "        observation = new_observation\n",
    "    \n",
    "    agent.optimize()\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return np.random.randint(low=0, high=1, size=(1))[0]\n",
    "\n",
    "def stick_policy(state):\n",
    "    player_score = state[0]\n",
    "    return 0 if player_score in [20, 21] else 1\n",
    "\n",
    "agent = FirstVisitMonteCarloPrediction(gamma=1, policy=stick_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100_000):\n",
    "    play_env(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.zeros(shape=(22, 12)) * np.nan\n",
    "\n",
    "for k in agent.state_value.keys():\n",
    "    Z[k[0]][k[1]] = agent.state_value[k]\n",
    "\n",
    "sh_0, sh_1 = Z.shape\n",
    "\n",
    "x, y = np.linspace(0, sh_1, sh_1), np.linspace(0, sh_0, sh_0)\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=Z, x=x, y=y)])\n",
    "\n",
    "fig.update_layout(title='MCFirstVisit',\n",
    "                  autosize=False,\n",
    "                  width=500, height=500,\n",
    "                  margin=dict(l=65, r=50, b=65, t=90))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Monte Carlo Estimation of Action Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloExploringStart():\n",
    "\n",
    "    def __init__(self, action_space, gamma, policy):\n",
    "        self.gamma  = gamma\n",
    "        self.policy = policy\n",
    "\n",
    "        self.state_action_value = collections.defaultdict(lambda: np.zeros((action_space.n)))\n",
    "        self.returns = collections.defaultdict(lambda: 0)\n",
    "\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "\n",
    "        self.first_action = True\n",
    "\n",
    "    def action(self, state):\n",
    "        if self.first_action:\n",
    "            self.first_action = False\n",
    "            return np.random.choice(len(self.state_action_value[state]))\n",
    "        else:\n",
    "            return self.policy(self.state_action_value, state)\n",
    "    \n",
    "    def observe(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def optimize(self):\n",
    "        g = 0\n",
    "\n",
    "        for t in reversed(range(len(self.states))):\n",
    "            g = self.gamma * g + self.rewards[t]\n",
    "\n",
    "            if not self.states[t] in self.states[0:t]:\n",
    "                self.returns[self.states[t]] += 1\n",
    "                self.state_action_value[self.states[t]][self.actions[t]] += (1 / self.returns[self.states[t]]) * (g - self.state_action_value[self.states[t]][self.actions[t]])\n",
    "        \n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "\n",
    "        self.first_action = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(state_action_value, state):\n",
    "    return np.argmax(state_action_value[state])\n",
    "\n",
    "def stick_policy(state_action_value, state):\n",
    "    player_score = state[0]\n",
    "    return 0 if player_score in [20, 21] else 1\n",
    "\n",
    "env = gym.make('Blackjack-v1', natural=True, sab=False)\n",
    "agent = MonteCarloExploringStart(action_space=env.action_space, gamma=1, policy=stick_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100_000):\n",
    "    play_env(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_v_from_q(state_action_value):\n",
    "    state_value = collections.defaultdict(lambda: np.ones((action_space.n)))\n",
    "\n",
    "    for s in state_action_value:\n",
    "        state_value[s] = np.mean(state_action_value[s])\n",
    "    \n",
    "    return state_value\n",
    "\n",
    "state_value = compute_v_from_q(agent.state_action_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.zeros(shape=(22, 12)) * np.nan\n",
    "\n",
    "for k in state_value.keys():\n",
    "    Z[k[0]][k[1]] = state_value[k]\n",
    "\n",
    "sh_0, sh_1 = Z.shape\n",
    "\n",
    "x, y = np.linspace(0, sh_1, sh_1), np.linspace(0, sh_0, sh_0)\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=Z, x=x, y=y)])\n",
    "\n",
    "fig.update_layout(title='Monte Carlo Exploring Start',\n",
    "                  autosize=False,\n",
    "                  width=500, height=500,\n",
    "                  margin=dict(l=65, r=50, b=65, t=90))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Control without exploring starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloExploringStart():\n",
    "\n",
    "    def __init__(self, action_space, gamma, policy):\n",
    "        self.gamma  = gamma\n",
    "        self.policy = policy\n",
    "\n",
    "        self.state_action_value = collections.defaultdict(lambda: np.zeros((action_space.n)))\n",
    "        self.returns = collections.defaultdict(lambda: 0)\n",
    "\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def action(self, state):\n",
    "        return self.policy(self.state_action_value, state)\n",
    "    \n",
    "    def observe(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def optimize(self):\n",
    "        g = 0\n",
    "\n",
    "        for t in reversed(range(len(self.states))):\n",
    "            g = self.gamma * g + self.rewards[t]\n",
    "\n",
    "            if not self.states[t] in self.states[0:t]:\n",
    "                self.returns[self.states[t]] += 1\n",
    "                self.state_action_value[self.states[t]][self.actions[t]] += (1 / self.returns[self.states[t]]) * (g - self.state_action_value[self.states[t]][self.actions[t]])\n",
    "        \n",
    "        self.states = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon_greedy_policy(epsilon=0.1):\n",
    "    def epsilon_greedy_policy(state_action_value, state):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < epsilon:\n",
    "            random_action = np.random.randint(0, len(state_action_value[state]))\n",
    "            return random_action\n",
    "        else:\n",
    "            greedy_action = argmax(state_action_value[state])\n",
    "            return greedy_action\n",
    "    \n",
    "    return epsilon_greedy_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy Prediction via Importance Sampling\n",
    "\n",
    "All learning control methods face a dilemma: They seek to learn action values conditional  \n",
    "on subsequent optimal behavior, but they need to behave non-optimally in order to  \n",
    "explore all actions (to find the optimal actions).\n",
    "\n",
    "A more straightforward approach is to use two policies:\n",
    " - **target policy** -> one that is learned about and that becomes the optimal policy\n",
    " - **behavior policy** -> one that is more exploratory and is used to generate behavior\n",
    "\n",
    "On-policy methods are generally simpler and are considered first.\n",
    "\n",
    "Off-policy methods require additional concepts and notation, and because the data is due to a different policy,  \n",
    "off-policy methods are often of greater variance and are slower to converge.\n",
    "\n",
    "On the other hand, off-policy methods are more powerful and general.\n",
    "\n",
    "For example, they can often be applied to learn from data generated by a conventional non-learning controller, or from a human expert.\n",
    "\n",
    "Off-policy learning is also seen by some as key to learning multi-step predictive models of the world’s dynamics\n",
    "\n",
    "$\\pi$ is the target policy\n",
    "$b$ is the behavior policy\n",
    "and both policies areconsidered fixed and given.\n",
    "\n",
    "It is require that $\\pi(a|s) > 0$ implies $b(a|s) > 0$.  \n",
    "This is called the assumption of **coverage**.\n",
    "\n",
    "Almost all off-policy methods utilize importance sampling, a general technique for  \n",
    "estimating expected values under one distribution given samples from another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(state_action_value):\n",
    "    e_x = np.exp(state_action_value - np.max(state_action_value))\n",
    "    probs = e_x / e_x.sum(axis=0)\n",
    "    return probs\n",
    "\n",
    "class OffPolicyMonteCarloPrediction():\n",
    "\n",
    "    def __init__(self, action_space, gamma, target_policy, behavior_policy):\n",
    "        self.gamma  = gamma\n",
    "\n",
    "        self.target_policy = target_policy\n",
    "        self.behavior_policy = behavior_policy\n",
    "\n",
    "        self.state_action_value = collections.defaultdict(lambda: np.zeros((action_space.n)))\n",
    "        self.state_action_value_behavior = collections.defaultdict(lambda: np.ones((action_space.n)) / action_space.n)\n",
    "\n",
    "        self.cumulative_weights = collections.defaultdict(lambda: np.zeros((action_space.n)))\n",
    "\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def action(self, state):\n",
    "        return self.behavior_policy(self.state_action_value_behavior, state)\n",
    "    \n",
    "    def observe(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def optimize(self):\n",
    "        g = 0\n",
    "        w = 1\n",
    "\n",
    "        for t in reversed(range(len(self.states))):\n",
    "            g = self.gamma * g + self.rewards[t]\n",
    "\n",
    "            self.cumulative_weights[self.states[t]][self.actions[t]] += w\n",
    "            self.state_action_value[self.states[t]][self.actions[t]] += (w / self.cumulative_weights[self.states[t]][self.actions[t]]) * (g - self.state_action_value[self.states[t]][self.actions[t]])\n",
    "\n",
    "            w = w * (softmax(self.state_action_value)[self.states[t]][self.actions[t]] / softmax(self.state_action_value_behavior)[self.states[t]][self.actions[t]])\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffPolicyMonteCarloControl():\n",
    "\n",
    "    def __init__(self, action_space, gamma, policy):\n",
    "        self.gamma  = gamma\n",
    "        self.policy = policy\n",
    "\n",
    "        self.state_action_value = collections.defaultdict(lambda: np.zeros((action_space.n)))\n",
    "        self.returns = collections.defaultdict(lambda: 0)\n",
    "\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def action(self, state):\n",
    "        return self.policy(self.state_action_value, state)\n",
    "    \n",
    "    def observe(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def optimize(self):\n",
    "        g = 0\n",
    "\n",
    "        for t in reversed(range(len(self.states))):\n",
    "            g = self.gamma * g + self.rewards[t]\n",
    "\n",
    "            if not self.states[t] in self.states[0:t]:\n",
    "                self.returns[self.states[t]] += 1\n",
    "                self.state_action_value[self.states[t]][self.actions[t]] += (1 / self.returns[self.states[t]]) * (g - self.state_action_value[self.states[t]][self.actions[t]])\n",
    "        \n",
    "        self.states = []\n",
    "        self.rewards = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
